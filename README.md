## ä¸€ä¸ªgithub actionsè‡ªåŠ¨ä»£ç†çˆ¬å–ä»“åº“

æ¯å¤©åŒ—äº¬æ—¶é—´10-12ç‚¹è¿è¡Œä¸€æ¬¡

### actionså½“å‰çŠ¶æ€

![Proxy Pool Update](https://github.com/LiMingda-101212/Proxy-Pool/actions/workflows/proxy-crawler.yml/badge.svg)

### ä»‹ç»

ä»£ç†æ± æ–‡ä»¶(csv)ä»‹ç»ï¼š

```
ç±»å‹,ä»£ç†,åˆ†æ•°,æ˜¯å¦æ”¯æŒä¸­å›½,æ˜¯å¦æ”¯æŒå›½é™…,æ˜¯å¦ä¸ºé€æ˜ä»£ç†,è¯†åˆ«åˆ°çš„ip
Type,Proxy:Port,Score,,China,International,Transparent,DetectedIP
```
æ”¯æŒhttp/socks4/socks5

#### åŠŸèƒ½ä»‹ç»:
æœ¬ç¨‹åºç”¨äºä»£ç†ç®¡ç†,æœ‰ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½:
1. åŠ è½½å’ŒéªŒè¯æ–°ä»£ç†,å¯ä»çˆ¬è™«(è‡ªåŠ¨),æœ¬åœ°æ–‡ä»¶(ç”¨äºæ‰‹åŠ¨æ·»åŠ ä»£ç†æ—¶ä½¿ç”¨,å¯ä»¥é€‰æ‹©ä»£ç†ç±»å‹(è¿™æ ·æ¯”è¾ƒå¿«),ä¹Ÿå¯ç”¨è‡ªåŠ¨æ£€æµ‹(è‹¥ç”¨è‡ªåŠ¨æ£€æµ‹å¯èƒ½è¾ƒæ…¢))åŠ è½½,å¹¶å°†é€šè¿‡çš„ä»£ç†æ·»åŠ åˆ°ä»£ç†æ± æ–‡ä»¶(OUTPUT_FILE).æ–°ä»£ç†ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹ç±»å‹æˆ–æŒ‡å®šç±»å‹.
  åœ¨éªŒè¯ä¹‹å‰ä¼šå…ˆå°†é‡å¤ä»£ç†,é”™è¯¯ä»£ç†ç­›é™¤,ç¡®ä¿ä¸åšæ— ç”¨åŠŸ.æ»¡åˆ†100åˆ†,æ–°ä»£ç†åªè¦é€šè¿‡ç™¾åº¦æˆ–Googleä»»ä¸€éªŒè¯å°±98åˆ†,é”™è¯¯ä»£ç†å’Œæ— æ•ˆä»£ç†0åˆ†(ä¼šè¢«0åˆ†æ¸…é™¤å‡½æ•°æ¸…é™¤).æ”¯æŒé€æ˜ä»£ç†æ£€æµ‹åŠŸèƒ½ï¼Œè¯†åˆ«ä¼šæ³„éœ²çœŸå®IPçš„ä»£ç†.
  æœ‰ä¸­æ–­æ¢å¤åŠŸèƒ½,å½“éªŒè¯è¿‡ç¨‹è¢«ä¸­æ–­æ—¶,ä¼šè‡ªåŠ¨ä¿å­˜å·²å®Œæˆçš„ä»£ç†åˆ°ä»£ç†æ± ,æœªå®Œæˆçš„ä»£ç†ä¿å­˜åˆ°ä¸­æ–­æ–‡ä»¶,ä¸‹æ¬¡å¯é€‰æ‹©ç»§ç»­éªŒè¯

2. æ£€éªŒå’Œæ›´æ–°ä»£ç†æ± å†…ä»£ç†çš„æœ‰æ•ˆæ€§,ä½¿ç”¨ä»£ç†æ± æ–‡ä»¶ä¸­çš„Typeä½œä¸ºç±»å‹,æœ€åä¸¤ä¸ªåˆ†åˆ«æ˜¯æ˜¯å¦æ”¯æŒå›½å†…å’Œå›½å¤–,å†æ¬¡éªŒè¯æˆåŠŸä¸€ä¸ª(å›½å†…/å›½å¤–)åŠ 1åˆ†,å…¨æˆåŠŸåŠ 2åˆ†,æ— æ•ˆä»£ç†å’Œé”™è¯¯ä»£ç†å‡1åˆ†,æ›´ç›´è§‚çš„åˆ†è¾¨ä»£ç†çš„ç¨³å®šæ€§.
  æ”¯æŒé€æ˜ä»£ç†æ£€æµ‹åŠŸèƒ½ï¼Œè¯†åˆ«ä¼šæ³„éœ²çœŸå®IPçš„ä»£ç†.æœ‰ä¸­æ–­æ¢å¤åŠŸèƒ½,å½“éªŒè¯è¿‡ç¨‹è¢«ä¸­æ–­æ—¶,ä¼šè‡ªåŠ¨ä¿å­˜å·²å®Œæˆçš„ä»£ç†åˆ°ä»£ç†æ± ,æœªå®Œæˆçš„ä»£ç†ä¿å­˜åˆ°ä¸­æ–­æ–‡ä»¶,ä¸‹æ¬¡å¯é€‰æ‹©ç»§ç»­éªŒè¯

3. æå–æŒ‡å®šæ•°é‡çš„ä»£ç†,ä¼˜å…ˆæå–åˆ†æ•°é«˜,ç¨³å®šçš„ä»£ç†,å¯æŒ‡å®šæå–ç±»å‹,æ”¯æŒèŒƒå›´å’Œæ˜¯å¦ä¸ºé€æ˜ä»£ç†
4. æŸ¥çœ‹ä»£ç†æ± çŠ¶æ€(æ€»ä»£ç†æ•°é‡,å„ç§ç±»å‹ä»£ç†çš„åˆ†æ•°åˆ†å¸ƒæƒ…å†µ,æ”¯æŒèŒƒå›´ç»Ÿè®¡)
5. æ”¯æŒé€æ˜ä»£ç†æ£€æµ‹åŠŸèƒ½ï¼Œè¯†åˆ«ä¼šæ³„éœ²çœŸå®IPçš„ä»£ç†

### å¿«é€Ÿå¼€å§‹

æœ¬åœ°ä½¿ç”¨:

ä¸‹æ–¹ç¨‹åºåŠŸèƒ½å®Œå–„,å®Œå…¨å¯ä»¥æœ¬åœ°ä½¿ç”¨

```python

import re
import requests
import concurrent.futures
import time
import os
import sys
import csv
import signal

# ============é»˜è®¤é…ç½®åŒº - Default Configuration
OUTPUT_FILE = "../ProxyPool/proxies.csv"  # è¾“å‡ºæœ‰æ•ˆä»£ç†æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰- Export valid proxy file (CSV format)
TEST_URL_CN = "http://www.baidu.com"  # å›½å†…æµ‹è¯•URL - Domestic test URL
TEST_URL_INTL = "http://www.google.com"  # å›½é™…æµ‹è¯•URL - International test URL
TRANSPARENT_CHECK_URL = "http://httpbin.org/ip"  # é€æ˜ä»£ç†æ£€æµ‹URL
TIMEOUT_CN = 6  # å›½å†…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’) - Domestic test timeout (s)
TIMEOUT_INTL = 10  # å›½é™…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’) - International test timeout (s)
TRANSPARENT_TIMEOUT = 8  # é€æ˜ä»£ç†æ£€æµ‹è¶…æ—¶æ—¶é—´(ç§’)
MAX_WORKERS = 100  # æœ€å¤§å¹¶å‘æ•° - Maximum concurrency
MAX_SCORE = 100  # æœ€å¤§ç§¯åˆ† - Maximum score

# ä¸­æ–­æ¢å¤ç›¸å…³é…ç½®
INTERRUPT_DIR = "../ProxyPool/interrupt"  # ä¸­æ–­æ–‡ä»¶ç›®å½•
INTERRUPT_FILE = os.path.join(INTERRUPT_DIR, "interrupted_proxies.csv")  # çˆ¬å–éªŒè¯ä¸­æ–­æ–‡ä»¶
INTERRUPT_FILE_LOAD = os.path.join(INTERRUPT_DIR, "interrupted_load_proxies.csv")   # æœ¬åœ°æ–‡ä»¶åŠ è½½ä¸­æ–­æ–‡ä»¶
INTERRUPT_FILE_EXISTING = os.path.join(INTERRUPT_DIR, "interrupted_existing_proxies.csv")   # æ›´æ–°ä»£ç†æ± ä¸­æ–­æ–‡ä»¶

# å…¨å±€å˜é‡ç”¨äºä¸­æ–­å¤„ç†
current_validation_process = None
interrupted = False

# çˆ¬å–å‚æ•°
HEADERS = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0'
}

def create_interrupt_dir():
    """åˆ›å»ºä¸­æ–­ç›®å½•"""
    os.makedirs(INTERRUPT_DIR, exist_ok=True)

def save_interrupted_proxies(remaining_proxies, proxy_type, original_count, interrupt_file=INTERRUPT_FILE):
    """ä¿å­˜ä¸­æ–­æ—¶çš„ä»£ç†åˆ—è¡¨"""
    create_interrupt_dir()
    with open(interrupt_file, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        writer.writerow([proxy_type, original_count])  # ç¬¬ä¸€è¡Œä¿å­˜ç±»å‹å’ŒåŸå§‹æ•°é‡
        for proxy in remaining_proxies:
            writer.writerow([proxy])

def load_interrupted_proxies(interrupt_file=INTERRUPT_FILE):
    """åŠ è½½ä¸­æ–­çš„ä»£ç†åˆ—è¡¨"""
    # å¦‚æœæ²¡æœ‰ä¸­æ–­è®°å½•
    if not os.path.exists(interrupt_file):
        return None, None, None
    
    try:
        with open(interrupt_file, 'r', encoding="utf-8") as file:
            reader = csv.reader(file)
            first_row = next(reader, None)
            # å¦‚æœæ— æ•ˆ
            if not first_row or len(first_row) < 2:
                return None, None, None
            
            proxy_type = first_row[0]
            original_count = int(first_row[1])
            remaining_proxies = [row[0] for row in reader if row]
        # æœ‰æ•ˆå¹¶æˆåŠŸè¯»å–  
        return remaining_proxies, proxy_type, original_count  # å‰©ä½™ä»£ç†,ç±»å‹,åŸå§‹æ•°é‡
    # å¤±è´¥
    except:
        return None, None, None

def delete_interrupt_file(interrupt_file=INTERRUPT_FILE):
    """åˆ é™¤ä¸­æ–­æ–‡ä»¶"""
    if os.path.exists(interrupt_file):
        os.remove(interrupt_file)

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºæ•è·Ctrl+C"""
    global interrupted
    interrupted = True
    print("\n\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")

def setup_interrupt_handler():
    """è®¾ç½®ä¸­æ–­å¤„ç†å™¨"""
    global interrupted
    interrupted = False
    signal.signal(signal.SIGINT, signal_handler)

class ProxyScraper:
    """
    get ip

    :param url: è¯·æ±‚åœ°å€
    :param regex_pattern: reè§£æå¼ï¼Œç”¨äºè§£æçˆ¬å–ç»“æœ
    :param capture_groups: è¦è¿”å›çš„reä¸­çš„å€¼ï¼Œ[IpName,Port]
    :return: [proxy:port]
    """
    def __init__(self, url: str, regex_pattern: str, capture_groups: list):
        self.url = url
        self.encoding = "utf-8"
        self.regex_pattern = regex_pattern
        self.capture_groups = capture_groups

    def scrape_proxies(self):
        extracted_data = []
        try:
            response = requests.get(url=self.url, headers=HEADERS, timeout=TIMEOUT_CN)
            if response.status_code == 200:  # åˆ¤æ–­çŠ¶æ€ç 
                response.encoding = self.encoding  # ä½¿ç”¨utf-8
                regex = re.compile(self.regex_pattern, re.S)  # åˆ›å»ºä¸€ä¸ªreå¯¹è±¡
                matches = regex.finditer(response.text)  # å¯¹è·å–çš„ä¸œè¥¿è¿›è¡Œè§£æ
                for match in matches:
                    for group_name in self.capture_groups:  # ä¾æ¬¡è¾“å‡ºå‚æ•°capture_groupsä¸­çš„æŒ‡å®šå†…å®¹
                        extracted_data.append(f"{match.group(group_name)}")
                proxy_list = [f"{extracted_data[i].strip()}:{extracted_data[i + 1].strip()}" for i in
                            range(0, len(extracted_data), 2)]  # æ•´åˆåˆ—è¡¨ä¸º[proxy:port]
                response.close()
                return proxy_list
            else:
                get_error = f"\nçˆ¬å–å¤±è´¥ï¼ŒâŒ çŠ¶æ€ç {response.status_code}"   # å‰é¢çš„\né˜²æ­¢ä¸è¿›åº¦æ¡æ··åœ¨ä¸€è¡Œ
                print(get_error)
                return get_error

        except Exception as e:
            get_error = f"\nçˆ¬å–å¤±è´¥ï¼ŒâŒ é”™è¯¯: {str(e)}"
            print(get_error)
            return get_error

def get_own_ip():
    """è·å–è‡ªå·±çš„å…¬ç½‘IPåœ°å€"""
    try:
        response = requests.get(TRANSPARENT_CHECK_URL, timeout=TRANSPARENT_TIMEOUT)
        if response.status_code == 200:
            return response.json()['origin']
    except Exception as e:
        print(f"è·å–æœ¬æœºIPå¤±è´¥: {str(e)}")
    return None

def check_transparent_proxy(proxy, proxy_type="http", own_ip=None):
    """
    æ£€æµ‹ä»£ç†æ˜¯å¦ä¸ºé€æ˜ä»£ç†
    
    :param proxy: ä»£ç†åœ°å€
    :param proxy_type: ä»£ç†ç±»å‹
    :param own_ip: è‡ªå·±çš„å…¬ç½‘IPï¼ˆå¯é€‰ï¼‰
    :return: (æ˜¯å¦ä¸ºé€æ˜ä»£ç†, æ£€æµ‹åˆ°çš„IP)
    """
    if own_ip is None:
        own_ip = get_own_ip()
        if own_ip is None:
            return False, "unknown"  # æ— æ³•è·å–æœ¬æœºIPï¼Œè·³è¿‡é€æ˜ä»£ç†æ£€æµ‹
    
    try:
        # è®¾ç½®ä»£ç†
        if proxy_type == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif proxy_type == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif proxy_type == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        
        # ä½¿ç”¨ä»£ç†è®¿é—®æ£€æµ‹ç½‘ç«™
        response = requests.get(
            TRANSPARENT_CHECK_URL,
            proxies=proxies_config,
            timeout=TRANSPARENT_TIMEOUT
        )
        
        if response.status_code == 200:
            proxy_ip_data = response.json()
            proxy_ip = proxy_ip_data['origin']
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé€æ˜ä»£ç†ï¼šå¦‚æœè¿”å›çš„IPåŒ…å«çœŸå®IPï¼Œåˆ™ä¸ºé€æ˜ä»£ç†
            is_transparent = own_ip in proxy_ip
            
            return is_transparent, proxy_ip
        else:
            return False, "unknown"
            
    except Exception as e:
        return False, "unknown"

def check_proxy_single(proxy, test_url, timeout=TIMEOUT_CN, 
                      retries=1, proxy_type="auto"):
    """
    æ£€æŸ¥å•ä¸ªä»£ç†IPå¯¹å•ä¸ªURLçš„å¯ç”¨æ€§ï¼ˆæ”¯æŒHTTPå’ŒSOCKSï¼‰
    
    :param proxy_type: ä»£ç†ç±»å‹ - "auto"(è‡ªåŠ¨æ£€æµ‹), "http", "socks4", "socks5"
    :param proxy: ä»£ç†IPåœ°å€å’Œç«¯å£ (æ ¼å¼: ip:port)
    :param test_url: ç”¨äºæµ‹è¯•çš„URL
    :param timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
    :param retries: é‡è¯•æ¬¡æ•°
    :return: æ˜¯å¦å¯ç”¨, å“åº”æ—¶é—´, æ£€æµ‹åˆ°çš„ç±»å‹
    """
    # æ ¹æ®ä»£ç†ç±»å‹è®¾ç½®proxieså­—å…¸
    if proxy_type == "auto":
        # è‡ªåŠ¨æ£€æµ‹ï¼šå…ˆå°è¯•HTTPï¼Œå†å°è¯•SOCKS5ï¼Œæœ€åSOCKS4
        protocols_to_try = ["http", "socks5", "socks4"]
    else:
        # æŒ‡å®šç±»å‹æ—¶ï¼Œåªå°è¯•è¯¥ç±»å‹
        protocols_to_try = [proxy_type]
    
    detected_type = proxy_type if proxy_type != "auto" else "unknown"
    
    for current_protocol in protocols_to_try:
        if current_protocol == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif current_protocol == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif current_protocol == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            continue

        for attempt in range(retries):
            try:
                start_time = time.time()
                response = requests.get(
                    test_url,
                    proxies=proxies_config,
                    timeout=timeout,
                    allow_redirects=False
                )
                end_time = time.time()
                response_time = end_time - start_time

                if response_time > timeout:
                    # è¶…æ—¶ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªåè®®ï¼ˆå¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹ï¼‰
                    break

                if response.status_code == 200:
                    detected_type = current_protocol
                    return True, response_time, detected_type
                    
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(0.5)
                    continue
                # å½“å‰åè®®å¤±è´¥ï¼Œå¦‚æœæ˜¯è‡ªåŠ¨æ£€æµ‹åˆ™å°è¯•ä¸‹ä¸€ä¸ªåè®®
                break
                
    # å¦‚æœæ˜¯æŒ‡å®šç±»å‹éªŒè¯å¤±è´¥ï¼Œè¿”å›æŒ‡å®šç±»å‹ï¼ˆå³ä½¿å¤±è´¥ï¼‰
    if proxy_type != "auto":
        detected_type = proxy_type
    
    return False, None, detected_type

def check_proxy_dual(proxy, proxy_type="auto", check_transparent=False):
    """
    åŒé‡éªŒè¯ä»£ç†ï¼šåŒæ—¶éªŒè¯ç™¾åº¦(å›½å†…)å’ŒGoogle(å›½é™…)ï¼Œå¯é€‰é€æ˜ä»£ç†æ£€æµ‹
    
    :return: (æ˜¯å¦é€šè¿‡å›½å†…, æ˜¯å¦é€šè¿‡å›½é™…, æœ€ç»ˆæ£€æµ‹ç±»å‹, æ˜¯å¦ä¸ºé€æ˜ä»£ç†, æ£€æµ‹åˆ°çš„IP)
    """
    # éªŒè¯å›½å†…ç½‘ç«™
    cn_success, cn_response_time, detected_type_cn = check_proxy_single(
        proxy, TEST_URL_CN, TIMEOUT_CN, 1, proxy_type
    )
    
    # éªŒè¯å›½é™…ç½‘ç«™  
    intl_success, intl_response_time, detected_type_intl = check_proxy_single(
        proxy, TEST_URL_INTL, TIMEOUT_INTL, 1, proxy_type
    )
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ£€æµ‹ç±»å‹ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªæ£€æµ‹ç±»å‹
    final_type = detected_type_cn if detected_type_cn != "unknown" else detected_type_intl
    if final_type == "unknown":
        final_type = proxy_type if proxy_type != "auto" else "http"
    
    # é€æ˜ä»£ç†æ£€æµ‹ï¼ˆåªåœ¨ä»£ç†æœ‰æ•ˆä¸”éœ€è¦æ£€æµ‹æ—¶è¿›è¡Œï¼‰
    is_transparent = False
    detected_ip = "unknown"
    
    if check_transparent and (cn_success or intl_success):
        is_transparent, detected_ip = check_transparent_proxy(proxy, final_type)
    
    return cn_success, intl_success, final_type, is_transparent, detected_ip

def check_proxies_batch(proxies, proxy_types, max_workers=MAX_WORKERS, check_type="new", check_transparent=True):
    """
    æ‰¹é‡æ£€æŸ¥ä»£ç†IPåˆ—è¡¨ï¼ˆåŒé‡éªŒè¯ + é€æ˜ä»£ç†æ£€æµ‹ï¼‰
    
    :param proxies: ä»£ç†å­—å…¸ {proxy: score}
    :param proxy_types: ä»£ç†ç±»å‹å­—å…¸ {proxy: type}
    :param check_type: "new" æ–°ä»£ç† / "existing" å·²æœ‰ä»£ç†
    :param check_transparent: æ˜¯å¦è¿›è¡Œé€æ˜ä»£ç†æ£€æµ‹
    """
    global interrupted
    
    updated_proxies = {}
    updated_types = {}
    updated_china = {}
    updated_international = {}
    updated_transparent = {}
    updated_detected_ips = {}

    # é¢„å…ˆè·å–æœ¬æœºIPç”¨äºé€æ˜ä»£ç†æ£€æµ‹
    own_ip = get_own_ip() if check_transparent else None
    if check_transparent and own_ip is None:
        print("âš ï¸  æ— æ³•è·å–æœ¬æœºIPï¼Œè·³è¿‡é€æ˜ä»£ç†æ£€æµ‹")
        check_transparent = False

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {}
        for proxy in proxies:
            if interrupted:
                break
                
            # å¯¹äºå·²æœ‰ä»£ç†ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­è®°å½•çš„ç±»å‹ï¼›å¯¹äºæ–°ä»£ç†ï¼Œå…ˆçœ‹æ˜¯å¦æŒ‡å®š,å¦åˆ™ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
            if check_type == "existing" and proxy in proxy_types:
                proxy_type = proxy_types[proxy]
            else:
                proxy_type = proxy_types.get(proxy, "auto")  # ä»ä¼ å…¥çš„ç±»å‹å­—å…¸è·å–
                
            future = executor.submit(check_proxy_dual, proxy, proxy_type, check_transparent)
            future_to_proxy[future] = proxy

        for future in concurrent.futures.as_completed(future_to_proxy):
            if interrupted:
                # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                for f in future_to_proxy:
                    f.cancel()
                break
                
            proxy = future_to_proxy[future]
            try:
                cn_success, intl_success, detected_type, is_transparent, detected_ip = future.result()

                # è®¡ç®—åˆ†æ•°å’Œæ›´æ–°é€»è¾‘
                current_score = proxies.get(proxy, 0)
                
                if check_type == "new":
                    # æ–°ä»£ç†ï¼šåªè¦é€šè¿‡ä»»ä¸€æµ‹è¯•å°±98åˆ†
                    if cn_success or intl_success:
                        updated_proxies[proxy] = 98
                        # é€æ˜ä»£ç†è­¦å‘Š
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: {'âœ“' if cn_success else 'âœ—'} å›½é™…: {'âœ“' if intl_success else 'âœ—'}{transparent_warning}")
                    else:
                        updated_proxies[proxy] = 0
                        print(f"âŒ ä»£ç†æ— æ•ˆ: {proxy}")
                else:
                    # å·²æœ‰ä»£ç†ï¼šæ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´åˆ†æ•°
                    if cn_success and intl_success:
                        # ä¸¤æ¬¡éƒ½é€šè¿‡ï¼ŒåŠ 2åˆ†
                        updated_proxies[proxy] = min(current_score + 2, MAX_SCORE)
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ“ å›½é™…: âœ“ | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}{transparent_warning}")
                    elif cn_success or intl_success:
                        # åªé€šè¿‡ä¸€ä¸ªï¼ŒåŠ 1åˆ†
                        updated_proxies[proxy] = min(current_score + 1, MAX_SCORE)
                        status = "å›½å†…: âœ“ å›½é™…: âœ—" if cn_success else "å›½å†…: âœ— å›½é™…: âœ“"
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(f"ğŸŸ¡ ä»£ç†éƒ¨åˆ†æœ‰æ•ˆ({detected_type}): {proxy} | {status} | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}{transparent_warning}")
                    else:
                        # ä¸¤ä¸ªéƒ½ä¸é€šè¿‡ï¼Œå‡1åˆ†
                        updated_proxies[proxy] = max(0, current_score - 1)
                        print(f"âŒ ä»£ç†æ— æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ— å›½é™…: âœ— | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}")
                
                # è®°å½•ç±»å‹å’Œæ”¯æŒèŒƒå›´
                updated_types[proxy] = detected_type
                updated_china[proxy] = cn_success
                updated_international[proxy] = intl_success
                updated_transparent[proxy] = is_transparent
                updated_detected_ips[proxy] = detected_ip
                        
            except Exception as e:
                if not interrupted:  # åªæœ‰ä¸æ˜¯ä¸­æ–­å¼•èµ·çš„å¼‚å¸¸æ‰æ‰“å°
                    print(f"âŒ é”™è¯¯ä»£ç†: {proxy} - {str(e)}")
                
                if check_type == "existing" and proxy in proxies:
                    updated_proxies[proxy] = max(0, proxies[proxy] - 1)
                else:
                    updated_proxies[proxy] = 0
                
                updated_types[proxy] = proxy_types.get(proxy, "http")
                updated_china[proxy] = False
                updated_international[proxy] = False
                updated_transparent[proxy] = False
                updated_detected_ips[proxy] = "unknown"
                    
    return updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips

def load_proxies_from_file(file_path):
    """ä»CSVæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨ã€ç±»å‹ã€åˆ†æ•°ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯"""
    proxies = {}
    proxy_types = {}
    china_support = {}
    international_support = {}
    transparent_proxies = {}
    detected_ips = {}
    
    if not os.path.exists(file_path):
        return proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips

    with open(file_path, 'r', encoding="utf-8") as file:
        reader = csv.reader(file)
        for row in reader:
            if len(row) >= 7:
                # ç±»å‹,proxy:port,åˆ†æ•°,China,International,Transparent,DetectedIP
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    china = row[3].strip().lower() == 'true'
                    international = row[4].strip().lower() == 'true'
                    transparent = row[5].strip().lower() == 'true'
                    detected_ip = row[6].strip() if len(row) > 6 else "unknown"
                    
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = china
                    international_support[proxy] = international
                    transparent_proxies[proxy] = transparent
                    detected_ips[proxy] = detected_ip
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
            elif len(row) >= 5:
                # æ—§æ ¼å¼å…¼å®¹ï¼šç±»å‹,proxy:port,åˆ†æ•°,China,Internationalï¼ˆé»˜è®¤éé€æ˜ä»£ç†ï¼‰
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    china = row[3].strip().lower() == 'true'
                    international = row[4].strip().lower() == 'true'
                    
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = china
                    international_support[proxy] = international
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
            elif len(row) >= 3:
                # æ›´æ—§æ ¼å¼å…¼å®¹ï¼šç±»å‹,proxy:port,åˆ†æ•°ï¼ˆé»˜è®¤ä¸æ”¯æŒä»»ä½•èŒƒå›´ï¼Œéé€æ˜ä»£ç†ï¼‰
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
            elif len(row) >= 2:
                # æœ€æ—§æ ¼å¼å…¼å®¹ï¼šproxy:port,åˆ†æ•°ï¼ˆé»˜è®¤HTTPç±»å‹ï¼Œä¸æ”¯æŒä»»ä½•èŒƒå›´ï¼Œéé€æ˜ä»£ç†ï¼‰
                proxy = row[0].strip()
                try:
                    score = int(row[1])
                    proxies[proxy] = score
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
    
    return proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips

def save_valid_proxies(proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips, file_path):
    """ä¿å­˜æœ‰æ•ˆä»£ç†åˆ°CSVæ–‡ä»¶ï¼ˆå¸¦ç±»å‹ã€åˆ†æ•°ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼‰"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    with open(file_path, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        for proxy, score in proxies.items():
            if len(proxy) > 6 and score > 0:  # åŸºæœ¬éªŒè¯
                proxy_type = proxy_types.get(proxy, "http")
                china = china_support.get(proxy, False)
                international = international_support.get(proxy, False)
                transparent = transparent_proxies.get(proxy, False)
                detected_ip = detected_ips.get(proxy, "unknown")
                writer.writerow([proxy_type, proxy, score, china, international, transparent, detected_ip])

def update_proxy_scores(file_path):
    """æ›´æ–°ä»£ç†åˆ†æ•°æ–‡ä»¶ï¼Œç§»é™¤0åˆ†ä»£ç†"""
    proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips = load_proxies_from_file(file_path)
    valid_proxies = {k: v for k, v in proxies.items() if v > 0}
    valid_types = {k: v for k, v in proxy_types.items() if k in valid_proxies}
    valid_china = {k: v for k, v in china_support.items() if k in valid_proxies}
    valid_international = {k: v for k, v in international_support.items() if k in valid_proxies}
    valid_transparent = {k: v for k, v in transparent_proxies.items() if k in valid_proxies}
    valid_detected_ips = {k: v for k, v in detected_ips.items() if k in valid_proxies}
    
    save_valid_proxies(valid_proxies, valid_types, valid_china, valid_international, valid_transparent, valid_detected_ips, file_path)
    return len(proxies) - len(valid_proxies)

def filter_proxies(all_proxies):
        """
        ä»æ–°è·å–ä»£ç†ä¸­å»æ‰æ— æ•ˆçš„,é‡å¤çš„
        :all_proxies: æ–°ä»£ç†åˆ—è¡¨
        :return: ç­›é€‰åçš„ä»£ç†åˆ—è¡¨
        """
        # è¿›è¡Œç­›é€‰
        existing_proxies = []
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE,'r') as file:
                csv_reader  = csv.reader(file)
                for row in csv_reader:
                    if len(row) >= 2:
                        existing_proxies.append(row[1])

        new_proxies = []
        duplicate_count = 0
        invalid_count = 0

        for proxy in all_proxies:
            try:
                if (proxy in existing_proxies) or (proxy in new_proxies):
                    print(f'â­•ï¸ å·²æœ‰ä»£ç†: {proxy}')
                    duplicate_count += 1
                elif (':' in proxy) and (proxy not in new_proxies):
                    new_proxies.append(proxy)
                    
                else:
                    print(f'âŒ æ ¼å¼æ— æ•ˆ: {proxy}')
                    invalid_count += 1
            except:
                invalid_count += 1

        print(f'æ–°ä»£ç†:{len(new_proxies)},å·²æœ‰(é‡å¤):{duplicate_count},æ— æ•ˆ:{invalid_count}')
        return new_proxies

def validate_new_proxies_with_interrupt(new_proxies, proxy_type="auto", from_interrupt=False, source="crawl", check_transparent=True):
    """éªŒè¯æ–°ä»£ç†ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤å’Œé€æ˜ä»£ç†æ£€æµ‹ï¼‰"""
    global interrupted
    
    if not new_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    # æ ¹æ®æ¥æºé€‰æ‹©ä¸­æ–­æ–‡ä»¶
    interrupt_file = INTERRUPT_FILE if source == "crawl" else INTERRUPT_FILE_LOAD
    
    original_count = len(new_proxies)
    print(f"å…±åŠ è½½ {original_count} ä¸ªæ–°ä»£ç†ï¼Œä½¿ç”¨{proxy_type}ç±»å‹å¼€å§‹åŒé‡æµ‹è¯•...")
    if check_transparent:
        print("ğŸ” å¯ç”¨é€æ˜ä»£ç†æ£€æµ‹")
    
    # ä¿å­˜åˆå§‹çŠ¶æ€åˆ°ä¸­æ–­æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯ä»ä¸­æ–­æ¢å¤çš„ï¼‰
    if not from_interrupt:
        save_interrupted_proxies(new_proxies, proxy_type, original_count, interrupt_file)
        print(f"ğŸ“ å·²åˆ›å»ºä¸­æ–­æ¢å¤æ–‡ä»¶: {interrupt_file}")
    
    # è®¾ç½®ä¸­æ–­å¤„ç†å™¨
    setup_interrupt_handler()
    
    # æ–°ä»£ç†åˆå§‹åˆ†æ•°ä¸º0
    new_proxies_dict = {proxy: 0 for proxy in new_proxies}
    new_types_dict = {proxy: proxy_type for proxy in new_proxies}
    
    try:
        updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips = check_proxies_batch(
            new_proxies_dict, new_types_dict, MAX_WORKERS, check_type="new", check_transparent=check_transparent
        )
        
        if interrupted:
            # è®¡ç®—å‰©ä½™æœªéªŒè¯çš„ä»£ç†
            verified_proxies = set(updated_proxies.keys())
            remaining_proxies = [proxy for proxy in new_proxies if proxy not in verified_proxies]
            
            # ä¿å­˜å·²éªŒè¯çš„ä»£ç†åˆ°ä»£ç†æ± 
            existing_proxies, existing_types, existing_china, existing_international, existing_transparent, existing_detected_ips = load_proxies_from_file(OUTPUT_FILE)
            for proxy, score in updated_proxies.items():
                if proxy not in existing_proxies or existing_proxies[proxy] < score:
                    existing_proxies[proxy] = score
                    existing_types[proxy] = updated_types[proxy]
                    existing_china[proxy] = updated_china[proxy]
                    existing_international[proxy] = updated_international[proxy]
                    existing_transparent[proxy] = updated_transparent[proxy]
                    existing_detected_ips[proxy] = updated_detected_ips[proxy]

            save_valid_proxies(existing_proxies, existing_types, existing_china, existing_international, existing_transparent, existing_detected_ips, OUTPUT_FILE)
            
            # æ›´æ–°ä¸­æ–­æ–‡ä»¶
            if remaining_proxies:
                save_interrupted_proxies(remaining_proxies, proxy_type, original_count, interrupt_file)
                print(f"\nâ¸ï¸ éªŒè¯å·²ä¸­æ–­ï¼å·²ä¿å­˜ {len(verified_proxies)} ä¸ªä»£ç†åˆ°ä»£ç†æ± ï¼Œå‰©ä½™ {len(remaining_proxies)} ä¸ªä»£ç†å¾…éªŒè¯")
                print(f"ğŸ“ ä¸­æ–­æ–‡ä»¶å·²æ›´æ–°: {interrupt_file}")
            else:
                delete_interrupt_file(interrupt_file)
                print(f"\nâœ… éªŒè¯å®Œæˆï¼æ‰€æœ‰ä»£ç†å·²éªŒè¯å¹¶ä¿å­˜")
                
            interrupted = False
            return
        
        # æ­£å¸¸å®ŒæˆéªŒè¯
        # åˆå¹¶åˆ°ç°æœ‰ä»£ç†æ± 
        existing_proxies, existing_types, existing_china, existing_international, existing_transparent, existing_detected_ips = load_proxies_from_file(OUTPUT_FILE)
        for proxy, score in updated_proxies.items():
            if proxy not in existing_proxies or existing_proxies[proxy] < score:
                existing_proxies[proxy] = score
                existing_types[proxy] = updated_types[proxy]
                existing_china[proxy] = updated_china[proxy]
                existing_international[proxy] = updated_international[proxy]
                existing_transparent[proxy] = updated_transparent[proxy]
                existing_detected_ips[proxy] = updated_detected_ips[proxy]

        save_valid_proxies(existing_proxies, existing_types, existing_china, existing_international, existing_transparent, existing_detected_ips, OUTPUT_FILE)
        
        # åˆ é™¤ä¸­æ–­æ–‡ä»¶
        delete_interrupt_file(interrupt_file)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for score in updated_proxies.values() if score == 98)
        china_only = sum(1 for proxy in updated_proxies if updated_china[proxy] and not updated_international[proxy])
        intl_only = sum(1 for proxy in updated_proxies if not updated_china[proxy] and updated_international[proxy])
        both_support = sum(1 for proxy in updated_proxies if updated_china[proxy] and updated_international[proxy])
        transparent_count = sum(1 for proxy in updated_proxies if updated_transparent[proxy])
        
        print(f"\nâœ… éªŒè¯å®Œæˆ!")
        print(f"æˆåŠŸä»£ç†: {success_count}/{original_count}")
        print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
        print(f"âš ï¸  é€æ˜ä»£ç†: {transparent_count} ä¸ª")
        print(f"ä»£ç†æ± å·²æ›´æ–°è‡³: {OUTPUT_FILE}")
        
    except Exception as e:
        if not interrupted:
            print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def validate_existing_proxies_with_interrupt(check_transparent=True):
    """éªŒè¯å·²æœ‰ä»£ç†æ± ä¸­çš„ä»£ç†ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤å’Œé€æ˜ä»£ç†æ£€æµ‹ï¼‰"""
    global interrupted
    
    print(f"å¼€å§‹éªŒè¯å·²æœ‰ä»£ç†æ± ï¼Œæ–‡ä»¶ï¼š{OUTPUT_FILE}...")
    if check_transparent:
        print("ğŸ” å¯ç”¨é€æ˜ä»£ç†æ£€æµ‹")
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
    remaining_proxies, _, original_count = load_interrupted_proxies(INTERRUPT_FILE_EXISTING)
    if remaining_proxies:
        print(f"ğŸ” å‘ç°ä¸Šæ¬¡éªŒè¯ä¸­æ–­è®°å½•!")
        print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
        print("\nè¯·é€‰æ‹©:")
        print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
        print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°éªŒè¯")
        print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
        
        choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
        
        if choice == 'y':
            print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
            proxies_to_validate = remaining_proxies
        elif choice == 'n':
            delete_interrupt_file(INTERRUPT_FILE_EXISTING)
            proxies_to_validate = None  # é‡æ–°åŠ è½½æ‰€æœ‰ä»£ç†
        else:
            print("è¿”å›ä¸Šçº§èœå•")
            return
    else:
        proxies_to_validate = None
    
    # åŠ è½½ä»£ç†æ± ï¼ˆä¸åŠ è½½æ—§çš„æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼Œä»¥æ–°éªŒè¯ç»“æœä¸ºå‡†ï¼‰
    all_proxies, proxy_types, _, _, _, _ = load_proxies_from_file(OUTPUT_FILE)
    
    if proxies_to_validate is None:
        # é‡æ–°éªŒè¯æ‰€æœ‰ä»£ç†
        proxies_to_validate = list(all_proxies.keys())
        original_count = len(proxies_to_validate)
    
    if not proxies_to_validate:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    print(f"å…±åŠ è½½ {len(proxies_to_validate)} ä¸ªä»£ç†ï¼Œå¼€å§‹åŒé‡æµ‹è¯•...")
    
    # ä¿å­˜åˆå§‹çŠ¶æ€åˆ°ä¸­æ–­æ–‡ä»¶
    save_interrupted_proxies(proxies_to_validate, "already_have", original_count, INTERRUPT_FILE_EXISTING)
    print(f"ğŸ“ å·²åˆ›å»ºä¸­æ–­æ¢å¤æ–‡ä»¶: {INTERRUPT_FILE_EXISTING}")
    
    # è®¾ç½®ä¸­æ–­å¤„ç†å™¨
    setup_interrupt_handler()
    
    try:
        # ä»ä»£ç†æ± ä¸­è·å–å½“å‰åˆ†æ•°å’Œç±»å‹ï¼ˆä¸è·å–æ—§çš„æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼‰
        proxies_dict = {proxy: all_proxies[proxy] for proxy in proxies_to_validate}
        types_dict = {proxy: proxy_types[proxy] for proxy in proxies_to_validate}
        
        updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips = check_proxies_batch(
            proxies_dict, types_dict, MAX_WORKERS, "existing", check_transparent
        )
        
        if interrupted:
            # è®¡ç®—å‰©ä½™æœªéªŒè¯çš„ä»£ç†
            verified_proxies = set(updated_proxies.keys())
            remaining_proxies = [proxy for proxy in proxies_to_validate if proxy not in verified_proxies]
            
            # æ›´æ–°å·²éªŒè¯çš„ä»£ç†åˆ†æ•°å’Œæ”¯æŒèŒƒå›´
            for proxy, score in updated_proxies.items():
                all_proxies[proxy] = score
                proxy_types[proxy] = updated_types[proxy]
            
            # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
            save_valid_proxies(all_proxies, proxy_types, updated_china, updated_international, updated_transparent, updated_detected_ips, OUTPUT_FILE)
            
            # æ›´æ–°ä¸­æ–­æ–‡ä»¶
            if remaining_proxies:
                save_interrupted_proxies(remaining_proxies, "already_have", original_count, INTERRUPT_FILE_EXISTING)
                print(f"\nâ¸ï¸ éªŒè¯å·²ä¸­æ–­ï¼å·²æ›´æ–° {len(verified_proxies)} ä¸ªä»£ç†ï¼Œå‰©ä½™ {len(remaining_proxies)} ä¸ªä»£ç†å¾…éªŒè¯")
                print(f"ğŸ“ ä¸­æ–­æ–‡ä»¶å·²æ›´æ–°: {INTERRUPT_FILE_EXISTING}")
            else:
                delete_interrupt_file(INTERRUPT_FILE_EXISTING)
                print(f"\nâœ… éªŒè¯å®Œæˆï¼æ‰€æœ‰ä»£ç†å·²æ›´æ–°")
                
            interrupted = False
            return
        
        # æ­£å¸¸å®ŒæˆéªŒè¯
        # æ›´æ–°æ‰€æœ‰ä»£ç†åˆ†æ•°å’Œæ”¯æŒèŒƒå›´
        for proxy, score in updated_proxies.items():
            all_proxies[proxy] = score
            proxy_types[proxy] = updated_types[proxy]
        
        # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
        save_valid_proxies(all_proxies, proxy_types, updated_china, updated_international, updated_transparent, updated_detected_ips, OUTPUT_FILE)
        
        # æ¸…ç†0åˆ†ä»£ç†
        removed_count = update_proxy_scores(OUTPUT_FILE)
        
        # åˆ é™¤ä¸­æ–­æ–‡ä»¶
        delete_interrupt_file(INTERRUPT_FILE_EXISTING)
        
        # æœ€ç»ˆç»Ÿè®¡
        final_proxies, _, final_china, final_international, final_transparent, _ = load_proxies_from_file(OUTPUT_FILE)
        final_count = len(final_proxies)
        
        china_only = sum(1 for proxy in final_proxies if final_china[proxy] and not final_international[proxy])
        intl_only = sum(1 for proxy in final_proxies if not final_china[proxy] and final_international[proxy])
        both_support = sum(1 for proxy in final_proxies if final_china[proxy] and final_international[proxy])
        transparent_count = sum(1 for proxy in final_proxies if final_transparent[proxy])

        print(f"\néªŒè¯å®Œæˆ! å‰©ä½™æœ‰æ•ˆä»£ç†: {final_count}/{original_count}")
        print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
        print(f"âš ï¸  é€æ˜ä»£ç†: {transparent_count} ä¸ª")
        print(f"å·²ç§»é™¤ {original_count - final_count} ä¸ªæ— æ•ˆä»£ç†")
        
    except Exception as e:
        if not interrupted:
            print(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

def extract_proxies_by_type(num, proxy_type="all", china_support=None, international_support=None, transparent_only=None):
    """
    æŒ‰ç±»å‹å’Œæ”¯æŒèŒƒå›´æå–æŒ‡å®šæ•°é‡çš„ä»£ç†ï¼Œä¼˜å…ˆæå–åˆ†é«˜çš„
    
    :param num: æ•°é‡
    :param proxy_type: ä»£ç†ç±»å‹ - "http", "socks4", "socks5", "all"
    :param china_support: æ˜¯å¦æ”¯æŒä¸­å›½ - True/False/None(ä¸é™åˆ¶)
    :param international_support: æ˜¯å¦æ”¯æŒå›½é™… - True/False/None(ä¸é™åˆ¶)
    :param transparent_only: æ˜¯å¦åªæå–é€æ˜ä»£ç† - True/False/None(ä¸é™åˆ¶)
    :return: ä»£ç†åˆ—è¡¨
    """
    proxies, proxy_types, china_support_dict, international_support_dict, transparent_proxies, _ = load_proxies_from_file(OUTPUT_FILE)
    
    # æŒ‰ç±»å‹å’Œæ”¯æŒèŒƒå›´ç­›é€‰
    filtered_proxies = {}
    for proxy, score in proxies.items():
        # ç±»å‹ç­›é€‰
        if proxy_type != "all" and proxy_types.get(proxy) != proxy_type:
            continue
            
        # ä¸­å›½æ”¯æŒç­›é€‰
        if china_support is not None and china_support_dict.get(proxy, False) != china_support:
            continue
            
        # å›½é™…æ”¯æŒç­›é€‰
        if international_support is not None and international_support_dict.get(proxy, False) != international_support:
            continue
            
        # é€æ˜ä»£ç†ç­›é€‰
        if transparent_only is not None and transparent_proxies.get(proxy, False) != transparent_only:
            continue
            
        filtered_proxies[proxy] = score

    # æŒ‰åˆ†æ•°é™åºæ’åº
    sorted_proxies = sorted(filtered_proxies.items(), key=lambda x: x[1], reverse=True)

    result = []
    for proxy, score in sorted_proxies:
        if len(result) >= num:
            break
        actual_type = proxy_types.get(proxy, "http")
        china = china_support_dict.get(proxy, False)
        international = international_support_dict.get(proxy, False)
        transparent = transparent_proxies.get(proxy, False)
        result.append({
            'proxy': f"{actual_type}://{proxy}",
            'score': score,
            'china': china,
            'international': international,
            'transparent': transparent
        })

    return result

def extract_proxies_menu():
    """æå–ä»£ç†èœå•ï¼ˆæ”¯æŒæŒ‰ç±»å‹ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ç­›é€‰ï¼‰"""
    try:
        count = int(input("è¯·è¾“å…¥è¦æå–çš„ä»£ç†æ•°é‡: ").strip())
        if count <= 0:
            print("æ•°é‡å¿…é¡»å¤§äº0")
            return

        # é€‰æ‹©ä»£ç†ç±»å‹
        print("\né€‰æ‹©ä»£ç†ç±»å‹:")
        print("1. http/https")
        print("2. socks4")
        print("3. socks5")
        print("4. å…¨éƒ¨ç±»å‹")
        type_choice = input("è¯·é€‰æ‹©(1-4): ").strip()
        
        type_map = {
            "1": "http",
            "2": "socks4", 
            "3": "socks5",
            "4": "all"
        }
        
        proxy_type = type_map.get(type_choice, "all")
        
        # é€‰æ‹©æ”¯æŒèŒƒå›´
        print("\né€‰æ‹©æ”¯æŒèŒƒå›´:")
        print("1. ä»…æ”¯æŒå›½å†…")
        print("2. ä»…æ”¯æŒå›½é™…") 
        print("3. æ”¯æŒå›½å†…å¤–")
        print("4. ä¸é™åˆ¶æ”¯æŒèŒƒå›´")
        support_choice = input("è¯·é€‰æ‹©(1-4): ").strip()
        
        china_support = None
        international_support = None
        
        if support_choice == "1":
            china_support = True
            international_support = False
        elif support_choice == "2":
            china_support = False  
            international_support = True
        elif support_choice == "3":
            china_support = True
            international_support = True
        # 4 å’Œå…¶ä»–æƒ…å†µä¸é™åˆ¶
        
        # é€‰æ‹©é€æ˜ä»£ç†ç­›é€‰
        print("\né€‰æ‹©é€æ˜ä»£ç†ç­›é€‰:")
        print("1. ä»…æå–é€æ˜ä»£ç†")
        print("2. ä»…æå–éé€æ˜ä»£ç†")
        print("3. ä¸é™åˆ¶")
        transparent_choice = input("è¯·é€‰æ‹©(1-3): ").strip()
        
        transparent_only = None
        if transparent_choice == "1":
            transparent_only = True
        elif transparent_choice == "2":
            transparent_only = False
        # 3 å’Œå…¶ä»–æƒ…å†µä¸é™åˆ¶
        
        proxies = extract_proxies_by_type(count, proxy_type, china_support, international_support, transparent_only)
        if not proxies:
            print("ä»£ç†æ± ä¸­æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ä»£ç†")
            return

        if len(proxies) < count:
            print(f"âš ï¸ è­¦å‘Š: åªæœ‰ {len(proxies)} ä¸ªç¬¦åˆæ¡ä»¶ä»£ç†ï¼Œå°‘äºè¯·æ±‚çš„ {count} ä¸ª")

        print(f"\næå–çš„ä»£ç†åˆ—è¡¨({proxy_type}):")
        for i, proxy_info in enumerate(proxies, 1):
            support_desc = []
            if proxy_info['china']:
                support_desc.append("å›½å†…")
            if proxy_info['international']:
                support_desc.append("å›½é™…")
            support_str = "|".join(support_desc) if support_desc else "æ— "
            transparent_str = "âš ï¸é€æ˜" if proxy_info['transparent'] else "åŒ¿å"
            print(f"{i}. {proxy_info['proxy']} | åˆ†æ•°:{proxy_info['score']} | æ”¯æŒ:{support_str} | {transparent_str}")

        save_choice = input("æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶? (y/n): ").lower().strip()
        if save_choice == "y":
            filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ")
            with open(filename, "w", encoding="utf-8") as file:
                for proxy_info in proxies:
                    file.write(f"{proxy_info['proxy']}\n")
            print(f"å·²ä¿å­˜åˆ° {filename}")
    except ValueError:
        print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

def show_proxy_pool_status():
    """æ˜¾ç¤ºä»£ç†æ± çŠ¶æ€ï¼ˆæŒ‰ç±»å‹ã€åˆ†æ•°ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ç»Ÿè®¡ï¼‰"""
    proxies, proxy_types, china_support, international_support, transparent_proxies, _ = load_proxies_from_file(OUTPUT_FILE)
    total = len(proxies)
    
    if total == 0:
        print("ä»£ç†æ± ä¸ºç©º")
        return

    # æŒ‰ç±»å‹åˆ†ç»„
    type_groups = {}
    for proxy, score in proxies.items():
        proxy_type = proxy_types.get(proxy, "http")
        if proxy_type not in type_groups:
            type_groups[proxy_type] = []
        type_groups[proxy_type].append((proxy, score, china_support.get(proxy, False), international_support.get(proxy, False), transparent_proxies.get(proxy, False)))

    print(f"\nä»£ç†æ± çŠ¶æ€ ({OUTPUT_FILE}):")
    print(f"æ€»ä»£ç†æ•°é‡: {total}")
    
    # æ”¯æŒèŒƒå›´ç»Ÿè®¡
    china_only = sum(1 for proxy in proxies if china_support.get(proxy, False) and not international_support.get(proxy, False))
    intl_only = sum(1 for proxy in proxies if not china_support.get(proxy, False) and international_support.get(proxy, False))
    both_support = sum(1 for proxy in proxies if china_support.get(proxy, False) and international_support.get(proxy, False))
    no_support = total - china_only - intl_only - both_support
    
    # é€æ˜ä»£ç†ç»Ÿè®¡
    transparent_count = sum(1 for proxy in proxies if transparent_proxies.get(proxy, False))
    anonymous_count = total - transparent_count
    
    print(f"\næ”¯æŒèŒƒå›´ç»Ÿè®¡:")
    print(f"  ä»…æ”¯æŒå›½å†…: {china_only}ä¸ª")
    print(f"  ä»…æ”¯æŒå›½é™…: {intl_only}ä¸ª") 
    print(f"  æ”¯æŒå›½å†…å¤–: {both_support}ä¸ª")
    print(f"  æ— æ”¯æŒ(æ— æ•ˆ): {no_support}ä¸ª")
    
    print(f"\né€æ˜ä»£ç†ç»Ÿè®¡:")
    print(f"  âš ï¸  é€æ˜ä»£ç†: {transparent_count}ä¸ª")
    print(f"  âœ… åŒ¿åä»£ç†: {anonymous_count}ä¸ª")
    
    # æŒ‰ç±»å‹æ˜¾ç¤ºç»Ÿè®¡
    for proxy_type, proxy_list in type_groups.items():
        type_count = len(proxy_list)
        print(f"\n{proxy_type.upper()} ä»£ç†: {type_count}ä¸ª")
        
        # ç»Ÿè®¡åˆ†æ•°åˆ†å¸ƒ
        score_count = {}
        for _, score, _, _, _ in proxy_list:
            score_count[score] = score_count.get(score, 0) + 1
        
        # æŒ‰åˆ†æ•°æ’åºæ˜¾ç¤º
        sorted_scores = sorted(score_count.items(), key=lambda x: x[0], reverse=True)
        for score, count in sorted_scores:
            print(f"  {score}åˆ†: {count}ä¸ª")
        
    print('='*40)
    print(f'æ€»è®¡: {total} ä¸ªä»£ç†')

def load_from_csv_with_type():
    """ä»CSVæ–‡ä»¶åŠ è½½å¹¶éªŒè¯ä»£ç†ï¼ˆæ”¯æŒç±»å‹é€‰æ‹©ï¼Œæ·»åŠ ä¸­æ–­æ¢å¤ï¼‰"""
    try:
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
        remaining_proxies, proxy_type, original_count = load_interrupted_proxies(INTERRUPT_FILE_LOAD)
        if remaining_proxies:
            print(f"ğŸ” å‘ç°ä¸Šæ¬¡æ–‡ä»¶åŠ è½½ä¸­æ–­è®°å½•!")
            print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
            print(f"   éªŒè¯ç±»å‹: {proxy_type}")
            print("\nè¯·é€‰æ‹©:")
            print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
            print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°é€‰æ‹©æ–‡ä»¶")
            print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
            
            choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
            
            if choice == 'y':
                print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
                validate_new_proxies_with_interrupt(remaining_proxies, proxy_type, from_interrupt=True, source="load")
                return
            elif choice == 'n':
                delete_interrupt_file(INTERRUPT_FILE_LOAD)
                print("å·²åˆ é™¤ä¸­æ–­è®°å½•ï¼Œå¼€å§‹é‡æ–°é€‰æ‹©æ–‡ä»¶...")
            else:
                print("è¿”å›ä¸Šçº§èœå•")
                return

        filename = input('æ–‡ä»¶å(è·¯å¾„): ')
        if not os.path.exists(filename):
            print("æ–‡ä»¶ä¸å­˜åœ¨")
            return
            
        # é€‰æ‹©ä»£ç†ç±»å‹
        print("\né€‰æ‹©ä»£ç†ç±»å‹:")
        print("1. http/https")
        print("2. socks4")
        print("3. socks5")
        print("4. è‡ªåŠ¨æ£€æµ‹")
        print("è¾“å…¥å…¶ä»–: ä½¿ç”¨é»˜è®¤å€¼http")
        type_choice = input("è¯·é€‰æ‹©(1-4): ").strip()
        
        type_map = {
            "1": "http",
            "2": "socks4",
            "3": "socks5",
            "4": "auto"
        }
        
        selected_type = type_map.get(type_choice, "http")
        print(f"ä½¿ç”¨ç±»å‹: {selected_type}")
        
        # é€‰æ‹©æ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†
        print("\næ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†?")
        print("1. æ˜¯ï¼ˆæ¨èï¼‰")
        print("2. å¦ï¼ˆæ›´å¿«ï¼‰")
        transparent_choice = input("è¯·é€‰æ‹©(1-2): ").strip()
        check_transparent = transparent_choice == "1"
        
        data = []
        with open(filename, 'r', encoding='utf-8') as file:
            reader = csv.reader(file)
            for row in reader:
                if len(row) >= 2:
                    # æ”¯æŒ ip,port æ ¼å¼
                    ip = row[0].strip()
                    port = row[1].strip()
                    if ip and port:
                        data.append(f"{ip}:{port}")
                elif len(row) == 1 and ':' in row[0]:
                    # æ”¯æŒ ip:port æ ¼å¼
                    data.append(row[0].strip())
        
        if not data:
            print("æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä»£ç†")
            return
            
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(data)} ä¸ªä»£ç†")
        
        # ç­›é€‰å»é‡
        new_proxies = filter_proxies(data)
        
        if new_proxies:
            if selected_type != "auto":
                # ä½¿ç”¨æŒ‡å®šç±»å‹éªŒè¯
                validate_new_proxies_with_interrupt(new_proxies, selected_type, source="load", check_transparent=check_transparent)
            else:
                # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
                validate_new_proxies_with_interrupt(new_proxies, "auto", source="load", check_transparent=check_transparent)
        else:
            print("æ²¡æœ‰æ–°ä»£ç†éœ€è¦éªŒè¯")
            
    except Exception as e:
        print(f'å‡ºé”™äº†: {str(e)}')

def download_from_github():
    """ä»GitHubä¸‹è½½ä»£ç†æ± å¹¶åˆå¹¶åˆ°æœ¬åœ°"""
    print("\nå¼€å§‹ä»GitHubä¸‹è½½ä»£ç†æ± ...")
    
    github_url = "https://raw.githubusercontent.com/LiMingda-101212/Proxy-Pool/refs/heads/main/proxies.csv"
    
    try:
        # ä¸‹è½½GitHubä¸Šçš„ä»£ç†æ± 
        response = requests.get(github_url, timeout=30)
        if response.status_code != 200:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return
        
        # è§£æGitHubä»£ç†æ± 
        github_proxies = {}
        github_types = {}
        github_china = {}
        github_international = {}
        github_transparent = {}
        github_detected_ips = {}
        
        content = response.text.strip().split('\n')
        reader = csv.reader(content)
        
        for row in reader:
            if len(row) >= 7:
                # æ–°æ ¼å¼ï¼šç±»å‹,proxy:port,åˆ†æ•°,China,International,Transparent,DetectedIP
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    china = row[3].strip().lower() == 'true'
                    international = row[4].strip().lower() == 'true'
                    transparent = row[5].strip().lower() == 'true'
                    detected_ip = row[6].strip() if len(row) > 6 else "unknown"
                    
                    github_proxies[proxy] = score
                    github_types[proxy] = proxy_type
                    github_china[proxy] = china
                    github_international[proxy] = international
                    github_transparent[proxy] = transparent
                    github_detected_ips[proxy] = detected_ip
                except Exception as e:
                    print(f"âŒ è§£æGitHubä»£ç†å¤±è´¥: {proxy} - {str(e)}")
                    continue
        
        print(f"âœ… ä»GitHubä¸‹è½½äº† {len(github_proxies)} ä¸ªä»£ç†")
        
        # åŠ è½½æœ¬åœ°ä»£ç†æ± 
        local_proxies, local_types, local_china, local_international, local_transparent, local_detected_ips = load_proxies_from_file(OUTPUT_FILE)
        
        # åˆå¹¶ä»£ç†æ± ï¼ˆä»¥GitHubä¸ºä¸»ï¼‰
        merged_count = 0
        updated_count = 0
        new_count = 0
        
        for proxy, score in github_proxies.items():
            if proxy in local_proxies:
                # ä»£ç†å·²å­˜åœ¨ï¼Œæ¯”è¾ƒåˆ†æ•°
                if score > local_proxies[proxy]:
                    local_proxies[proxy] = score
                    local_types[proxy] = github_types[proxy]
                    local_china[proxy] = github_china[proxy]
                    local_international[proxy] = github_international[proxy]
                    local_transparent[proxy] = github_transparent[proxy]
                    local_detected_ips[proxy] = github_detected_ips[proxy]
                    updated_count += 1
                merged_count += 1
            else:
                # æ–°ä»£ç†
                local_proxies[proxy] = score
                local_types[proxy] = github_types[proxy]
                local_china[proxy] = github_china[proxy]
                local_international[proxy] = github_international[proxy]
                local_transparent[proxy] = github_transparent[proxy]
                local_detected_ips[proxy] = github_detected_ips[proxy]
                new_count += 1
        
        # ä¿å­˜åˆå¹¶åçš„ä»£ç†æ± 
        save_valid_proxies(local_proxies, local_types, local_china, local_international, local_transparent, local_detected_ips, OUTPUT_FILE)
        
        print(f"\nâœ… åˆå¹¶å®Œæˆ!")
        print(f"æ€»ä»£ç†æ•°: {len(local_proxies)}")
        print(f"å·²å­˜åœ¨ä»£ç†: {merged_count}")
        print(f"æ›´æ–°ä»£ç†: {updated_count}")
        print(f"æ–°å¢ä»£ç†: {new_count}")
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")

def crawl_proxies():
    """çˆ¬å–å…è´¹ä»£ç†ï¼ˆæ·»åŠ ä¸­æ–­æ¢å¤æ£€æŸ¥ï¼‰"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è®°å½•
    remaining_proxies, proxy_type, original_count = load_interrupted_proxies(INTERRUPT_FILE)
    if remaining_proxies:
        print(f"ğŸ” å‘ç°ä¸Šæ¬¡ä¸­æ–­è®°å½•!")
        print(f"   å‰©ä½™ä»£ç†: {len(remaining_proxies)}/{original_count} ä¸ª")
        print(f"   éªŒè¯ç±»å‹: {proxy_type}")
        print("\nè¯·é€‰æ‹©:")
        print("  y: ç»§ç»­ä¸Šæ¬¡éªŒè¯")
        print("  n: åˆ é™¤è®°å½•å¹¶é‡æ–°çˆ¬å–")
        print("  å…¶ä»–: è¿”å›ä¸Šçº§èœå•")
        
        choice = input("è¯·é€‰æ‹© (y/n/å…¶ä»–): ").lower().strip()
        
        if choice == 'y':
            print("ç»§ç»­ä¸Šæ¬¡éªŒè¯...")
            return remaining_proxies, proxy_type
        elif choice == 'n':
            delete_interrupt_file(INTERRUPT_FILE)
            print("å·²åˆ é™¤ä¸­æ–­è®°å½•ï¼Œå¼€å§‹é‡æ–°çˆ¬å–...")
        else:
            print("è¿”å›ä¸Šçº§èœå•")
            return None, None

    print("""å·²åˆ›å»ºçš„å¯çˆ¬ç½‘ç«™
    1 ï¼šhttps://proxy5.net/cn/free-proxy/china
          å¤‡æ³¨:è¢«å°äº†,æˆåŠŸç‡ 40%
    2 ï¼šhttps://www.89ip.cn/
          å¤‡æ³¨:240ä¸ª,æˆåŠŸç‡ 10%
    3 ï¼šhttps://cn.freevpnnode.com/
          å¤‡æ³¨:30ä¸ª,æˆåŠŸç‡ 3%
    4 ï¼šhttps://www.kuaidaili.com/free/inha/ 
          å¤‡æ³¨:7600å¤šé¡µ,æˆåŠŸç‡ 5%
    5 ï¼šhttp://www.ip3366.net/
          å¤‡æ³¨:100ä¸ª,æˆåŠŸç‡ 1%
    6 ï¼šhttps://proxypool.scrape.center/random
          å¤‡æ³¨:éšæœºçš„,æˆåŠŸç‡ 40%
    7 ï¼šhttps://proxy.scdn.io/text.php
          å¤‡æ³¨:12000å¤šä¸ª,æˆåŠŸç‡ 30%
    8 ï¼šhttps://proxyhub.me/zh/cn-http-proxy-list.html
          å¤‡æ³¨:20ä¸ª,æˆåŠŸç‡ 0%
    9 : https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt
          å¤‡æ³¨:å¤§çº¦3000ä¸ª,æˆåŠŸç‡ 15%
    10: https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt
          å¤‡æ³¨:å¤§çº¦2000ä¸ª,æˆåŠŸç‡ 10%
    11: https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt
          å¤‡æ³¨:å¤§çº¦3000ä¸ª,æˆåŠŸç‡ 10%
    12: https://github.com/zloi-user/hideip.me/raw/refs/heads/master/http.txt
          å¤‡æ³¨:å¤§çº¦1000ä¸ª,æˆåŠŸç‡ 20%
    13: https://github.com/zloi-user/hideip.me/raw/refs/heads/master/https.txt
          å¤‡æ³¨:å¤§çº¦1000ä¸ª,æˆåŠŸç‡ 0%
    14: https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks4.txt
          å¤‡æ³¨:å¤§çº¦100ä¸ª,æˆåŠŸç‡ 30%
    15: https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks5.txt
          å¤‡æ³¨:å¤§çº¦50ä¸ª,æˆåŠŸç‡ 30% 
    16: https://raw.githubusercontent.com/r00tee/Proxy-List/main/Https.txt
          å¤‡æ³¨:å¤§çº¦50000ä¸ª,æˆåŠŸç‡ 0.001%
    17: https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks4.txt
          å¤‡æ³¨:å¤§çº¦50000ä¸ª,æˆåŠŸç‡ 0.001%
    18: https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt
          å¤‡æ³¨:å¤§çº¦4000ä¸ª,æˆåŠŸç‡ 0.01%
          
    è¾“å…¥å…¶ä»–ï¼šé€€å‡º
    """)
    scraper_choice = input("é€‰æ‹©ï¼š").strip()
    all_proxies = []  # å­˜å‚¨æ‰€æœ‰çˆ¬å–çš„ä»£ç†
    by_type = ''  # é€šè¿‡æŒ‡å®šç±»å‹éªŒè¯,é»˜è®¤ä¸ºå¦

    if scraper_choice == "1":
        print('å¼€å§‹çˆ¬å–:https://proxy5.net/cn/free-proxy/china')
        error_count = 0
        '''
        <tr>.*?<td><strong>(?P<ip>.*?)</strong></td>.*?<td>(?P<port>.*?)</td>.*?</tr>
        '''
        proxy_list = ProxyScraper('https://proxy5.net/cn/free-proxy/china',
                        "<tr>.*?<td><strong>(?P<ip>.*?)</strong></td>.*?<td>(?P<port>.*?)</td>.*?</tr>",
                        ["ip", "port"]).scrape_proxies()
        
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')

    elif scraper_choice == "2":
        print('å¼€å§‹çˆ¬å–:https://www.89ip.cn/')
        error_count = 0
        total_pages = 6
        for page in range(1, total_pages+1):
            if page == 1:
                url = 'https://www.89ip.cn/'
            else:
                url = f'https://www.89ip.cn/index_{page}.html'

            proxy_list = ProxyScraper(url,"<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?</tr>",
                            ["ip", "port"]).scrape_proxies()
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1

            time.sleep(1)

            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            percent = page * 100 // total_pages
            # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
            completed = page * 50 // total_pages
            remaining = 50 - completed
            # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
            if percent < 10:
                padding = "  "
            elif percent < 100:
                padding = " "
            else:
                padding = ""
            # æ›´æ–°è¿›åº¦æ¡
            print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
            sys.stdout.flush()
        print('\n')
        
    elif scraper_choice == "3":
        print('\nå¼€å§‹çˆ¬å–:https://cn.freevpnnode.com/')
        error_count = 0
        proxy_list = ProxyScraper("https://cn.freevpnnode.com/",
                        '<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?<td><span>.*?</span> <img src=".*?" width="20" height="20" .*? class="js_openeyes"></td>.*?</td>',
                        ["ip", "port"]).scrape_proxies()
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')

    elif scraper_choice == "4":
        error_count = 0
        try:
            print('ä¿¡æ¯:å…±çº¦7000é¡µ,å»ºè®®ä¸€æ¬¡çˆ¬å–æ•°é‡ä¸å¤§äº500é¡µ,é˜²æ­¢è¢«å°')
            start_page = int(input('çˆ¬å–èµ·å§‹é¡µï¼ˆæ•´æ•°ï¼‰ï¼š').strip())
            end_page = int(input("çˆ¬å–ç»“æŸé¡µï¼ˆæ•´æ•°ï¼‰:").strip())
            if end_page < 1 or start_page < 1 or end_page > 7000 or start_page > 7000 or start_page > end_page:
                print("ä¸èƒ½å°äº1æˆ–å¤§äº7000,èµ·å§‹é¡µä¸èƒ½å¤§äºç»“æŸé¡µ")
                return

            print('å¼€å§‹çˆ¬å–:https://www.kuaidaili.com/free/inha/')
            
            for page in range(start_page, end_page + 1):

                proxy_list = ProxyScraper(f"https://www.kuaidaili.com/free/inha/{page}/",
                                '{"ip": "(?P<ip>.*?)", "last_check_time": ".*?", "port": "(?P<port>.*?)", "speed": .*?, "location": ".*?"}',
                                ["ip", "port"]).scrape_proxies()
                if isinstance(proxy_list, list):
                    all_proxies.extend(proxy_list)
                else:
                    error_count += 1

                time.sleep(2)

                # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                current_page = page - start_page + 1
                total_pages = end_page - start_page + 1
                percent = current_page * 100 // total_pages
                # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
                completed = current_page * 50 // total_pages
                remaining = 50 - completed
                # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
                if percent < 10:
                    padding = "  "
                elif percent < 100:
                    padding = " "
                else:
                    padding = ""
                # æ›´æ–°è¿›åº¦æ¡
                print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {current_page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
                sys.stdout.flush()
            print('\n')
        except:
            print("è¾“å…¥é”™è¯¯ï¼Œè¯·è¾“å…¥æ•´æ•°")

    elif scraper_choice == "5":

        print('\nå¼€å§‹çˆ¬å–:http://www.ip3366.net/?stype=1')
        total_pages = 7
        error_count = 0
        for page in range(1, total_pages + 1):
            proxy_list = ProxyScraper(f'http://www.ip3366.net/?stype=1&page={page}',
                            '<tr>.*?<td>(?P<ip>.*?)</td>.*?<td>(?P<port>.*?)</td>.*?</tr>', ['ip', 'port']).scrape_proxies()
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1

            time.sleep(1)

            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            percent = page * 100 // total_pages
            # è®¡ç®—è¿›åº¦æ¡é•¿åº¦
            completed = page * 50 // total_pages
            remaining = 50 - completed
            # å¤„ç†ç™¾åˆ†æ¯”æ˜¾ç¤ºçš„å¯¹é½
            if percent < 10:
                padding = "  "
            elif percent < 100:
                padding = " "
            else:
                padding = ""
            # æ›´æ–°è¿›åº¦æ¡
            print(f"\r{percent}%{padding}|{'â–ˆ' * completed}{'-' * remaining}| {page}/{total_pages}  é”™è¯¯æ•°:{error_count}", end="")
            sys.stdout.flush()
        print('\n')

    elif scraper_choice == "6":
        try:
            by_type = 'http'   # é»˜è®¤ç”¨http
            count = int(input("çˆ¬å–ä¸ªæ•°(æ•´æ•°)ï¼š").strip())
            if count < 1:
                print("æ•°é‡å¿…é¡»å¤§äº0")
                return None,None

            print(f"\nå¼€å§‹çˆ¬å– {count} ä¸ªä»£ç†...")
        
            try:
                # é€‚åˆç”¨åç¨‹
                import aiohttp
                import asyncio
                
                async def fetch_proxy(session, url, semaphore):
                    async with semaphore:
                        try:
                            async with session.get(url) as response:
                                if response.status == 200:
                                    proxy = await response.text()
                                    print(proxy)
                                    return proxy.strip()
                        except:
                            return None
                
                async def fetch_proxies_main():
                    semaphore = asyncio.Semaphore(20)   # æœ€å¤§å¹¶å‘
                    timeout = aiohttp.ClientTimeout(total=50)   # è¶…æ—¶(ç»™æœåŠ¡å™¨è¶³å¤Ÿå“åº”æ—¶é—´)
                    
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        tasks = []
                        for _ in range(count):
                            url = 'https://proxypool.scrape.center/random'
                            task = fetch_proxy(session, url, semaphore)
                            tasks.append(task)
                        
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        return [r for r in results if r and isinstance(r, str) and ':' in r]
                
                proxies = asyncio.run(fetch_proxies_main())
                if proxies:
                    all_proxies.extend(proxies)
                    
            except ImportError:
                print("aiohttp æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥è¯·æ±‚...")
                # åŒæ­¥å¤‡é€‰æ–¹æ¡ˆ
                for _ in range(count):
                    try:
                        proxy = requests.get('https://proxypool.scrape.center/random', timeout=10).text.strip()
                        if proxy and ':' in proxy:
                            all_proxies.append(proxy)
                    except:
                        continue

            print(f"\nçˆ¬å–å®Œæˆï¼")
        except ValueError:
            print("è¾“å…¥é”™è¯¯")
            return None, None

    elif scraper_choice == '7':
        print("\nå¼€å§‹çˆ¬å–:https://proxy.scdn.io/text.php")
        error_count = 0
        url = 'https://proxy.scdn.io/text.php'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
            'Referer':'https://proxy.scdn.io/'
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')
    
    elif scraper_choice == '8':
        print('\nå¼€å§‹çˆ¬å–:https://proxyhub.me/zh/cn-http-proxy-list.html')
        error_count = 0
        proxy_list = ProxyScraper("https://proxyhub.me/zh/cn-http-proxy-list.html",
                        r'<tr>\s*<td>(?P<ip>\d+\.\d+\.\d+\.\d+)</td>\s*<td>(?P<port>\d+)</td>',
                        ["ip", "port"]).scrape_proxies()
        if isinstance(proxy_list, list):
            all_proxies.extend(proxy_list)
        else:
            error_count += 1
        print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')

    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt
    elif scraper_choice == '9':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '10':
        by_type = 'socks5'   # é»˜è®¤ç”¨socks5
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt'
        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '11':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt'
        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks4.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt
    elif scraper_choice == '12':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '13':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '14':
        by_type = 'socks4'
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '15':
        by_type = 'socks5'
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://raw.githubusercontent.com/r00tee/Proxy-List/main/Https.txt
    # https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks4.txt
    # https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt
    elif scraper_choice == '16':
        by_type = 'http'
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/r00tee/Proxy-List/main/Https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/r00tee/Proxy-List/main/Https.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')
    
    elif scraper_choice == '17':
        by_type = 'socks4'
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks4.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks4.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')
    
    elif scraper_choice == '18':
        by_type = 'socks5'
        print('\nå¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt'

        try:
            response = requests.get(url,headers=HEADERS)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')


    # https://raw.githubusercontent.com/Zaeem20/FREE_PROXIES_LIST/refs/heads/master/http.txt -> è´¨é‡å¾ˆå·®,æš‚æ—¶ä¸æ·»åŠ 

    # https://github.com/FifzzSENZE/Master-Proxy.git -> è´¨é‡ä¸€èˆ¬,æš‚æ—¶ä¸æ·»åŠ 
    # https://github.com/dpangestuw/Free-Proxy.git -> è´¨é‡ä¸€èˆ¬,æš‚æ—¶ä¸æ·»åŠ 
    # https://github.com/watchttvv/free-proxy-list.git -> å¯ä»¥,ä½†æ¯”è¾ƒå°‘
    # https://github.com/trio666/proxy-checker.git
    
    return filter_proxies(all_proxies), by_type

if __name__ == '__main__':
    # åˆ›å»ºä¸­æ–­ç›®å½•
    create_interrupt_dir()

    while True:
        print(f"""åŠŸèƒ½ï¼š
        1: åŠ è½½å¹¶éªŒè¯æ–°ä»£ç† (æˆåŠŸåæ·»åŠ åˆ°ä»£ç†æ± )
        2: æ£€éªŒå¹¶æ›´æ–°å·²æœ‰ä»£ç†
        3: æå–ä»£ç†(å¯æŒ‡å®šæ•°é‡,ç±»å‹,æ”¯æŒèŒƒå›´,é€æ˜ä»£ç†)
        4: æŸ¥çœ‹ä»£ç†æ± çŠ¶æ€
        5: åŒæ­¥ä»£ç†æ± ï¼ˆGitHubï¼‰


        è¾“å…¥å…¶ä»–: é€€å‡º
        """)
        choice = input("é€‰æ‹©ï¼š").strip()

        if choice == "1":
            print('''æ¥è‡ª:
                  1: æ¥è‡ªçˆ¬è™«çˆ¬å–
                  2: æ¥è‡ªæœ¬åœ°æ–‡ä»¶(proxy,port)

                  è¾“å…¥å…¶ä»–: è¿”å›ä¸Šçº§èœå•
            ''')
            from_choice = input('é€‰æ‹©:').strip()

            if from_choice == '1':
                new_proxies, by_type = crawl_proxies()
                if new_proxies:   # å¦‚æœæœ‰æ–°ä»£ç†
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä»ä¸­æ–­æ¢å¤çš„
                    remaining_proxies, interrupt_type, _ = load_interrupted_proxies()
                    from_interrupt = remaining_proxies is not None and remaining_proxies == new_proxies
                    
                    # è¯¢é—®æ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†
                    print("\næ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†?")
                    print("1. æ˜¯ï¼ˆæ¨èï¼Œè¯†åˆ«ä¼šæ³„éœ²çœŸå®IPçš„ä»£ç†ï¼‰")
                    print("2. å¦ï¼ˆæ›´å¿«ï¼‰")
                    transparent_choice = input("è¯·é€‰æ‹©(1-2): ").strip()
                    check_transparent = transparent_choice == "1"
                    
                    if by_type:   # å¦‚æœæŒ‡å®šç±»å‹
                        validate_new_proxies_with_interrupt(new_proxies, by_type, from_interrupt, check_transparent=check_transparent)
                    else:   # æ²¡æœ‰æŒ‡å®šç±»å‹
                        validate_new_proxies_with_interrupt(new_proxies, "auto", from_interrupt, check_transparent=check_transparent)

            elif from_choice == '2':
                load_from_csv_with_type()

            else:
                print('è¿”å›ä¸Šçº§èœå•')
                continue

        elif choice == "2":
            # è¯¢é—®æ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†
            print("\næ˜¯å¦æ£€æµ‹é€æ˜ä»£ç†?")
            print("1. æ˜¯ï¼ˆæ¨èï¼Œè¯†åˆ«ä¼šæ³„éœ²çœŸå®IPçš„ä»£ç†ï¼‰")
            print("2. å¦ï¼ˆæ›´å¿«ï¼‰")
            transparent_choice = input("è¯·é€‰æ‹©(1-2): ").strip()
            check_transparent = transparent_choice == "1"
            
            validate_existing_proxies_with_interrupt(check_transparent)

        elif choice == "3":
            extract_proxies_menu()

        elif choice == "4":
            show_proxy_pool_status()
        
        elif choice == "5":
            download_from_github()

        else:
            print('é€€å‡º')
            break

```
