import re
import requests
import concurrent.futures
import time
import os
import csv

# ============é…ç½®åŒº
OUTPUT_FILE = "proxies.csv"  # è¾“å‡ºæœ‰æ•ˆä»£ç†æ–‡ä»¶
TEST_URL_CN = "http://www.baidu.com"  # å›½å†…æµ‹è¯•URL
TEST_URL_INTL = "http://www.google.com"  # å›½é™…æµ‹è¯•URL
TIMEOUT_CN = 6  # å›½å†…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)
TIMEOUT_INTL = 10  # å›½é™…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)
MAX_WORKERS = 50  # æœ€å¤§å¹¶å‘æ•°
MAX_SCORE = 100  # æœ€å¤§ç§¯åˆ†

class ProxyScraper:
    """ä»£ç†çˆ¬å–å™¨"""
    def __init__(self, url: str, regex_pattern: str, capture_groups: list):
        self.url = url
        self.headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0'
        }
        self.encoding = "utf-8"
        self.regex_pattern = regex_pattern
        self.capture_groups = capture_groups

    def scrape_proxies(self):
        extracted_data = []
        try:
            response = requests.get(url=self.url, headers=self.headers, timeout=TIMEOUT_CN)
            if response.status_code == 200:
                response.encoding = self.encoding
                regex = re.compile(self.regex_pattern, re.S)
                matches = regex.finditer(response.text)
                for match in matches:
                    for group_name in self.capture_groups:
                        extracted_data.append(f"{match.group(group_name)}")
                proxy_list = [f"{extracted_data[i].strip()}:{extracted_data[i + 1].strip()}" 
                            for i in range(0, len(extracted_data), 2)]
                response.close()
                return proxy_list
            else:
                print(f"çˆ¬å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥ï¼Œé”™è¯¯: {str(e)}")
            return []

def check_proxy_single(proxy, test_url, timeout=TIMEOUT_CN, 
                      retries=1, proxy_type="auto"):
    """
    æ£€æŸ¥å•ä¸ªä»£ç†IPå¯¹å•ä¸ªURLçš„å¯ç”¨æ€§
    """
    if proxy_type == "auto":
        protocols_to_try = ["http", "socks5", "socks4"]
    else:
        protocols_to_try = [proxy_type]
    
    detected_type = proxy_type if proxy_type != "auto" else "unknown"
    
    for current_protocol in protocols_to_try:
        if current_protocol == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif current_protocol == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif current_protocol == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            continue

        for attempt in range(retries):
            try:
                start_time = time.time()
                response = requests.get(
                    test_url,
                    proxies=proxies_config,
                    timeout=timeout,
                    allow_redirects=False
                )
                end_time = time.time()
                response_time = end_time - start_time

                if response_time > timeout:
                    break

                if response.status_code == 200:
                    detected_type = current_protocol
                    return True, response_time, detected_type
                    
            except Exception:
                if attempt < retries - 1:
                    time.sleep(0.5)
                    continue
                break
                
    if proxy_type != "auto":
        detected_type = proxy_type
    
    return False, None, detected_type

def check_proxy_dual(proxy, proxy_type="auto"):
    """
    åŒé‡éªŒè¯ä»£ç†ï¼šåŒæ—¶éªŒè¯ç™¾åº¦(å›½å†…)å’ŒGoogle(å›½é™…)
    
    :return: (æ˜¯å¦é€šè¿‡å›½å†…, æ˜¯å¦é€šè¿‡å›½é™…, æœ€ç»ˆæ£€æµ‹ç±»å‹)
    """
    # éªŒè¯å›½å†…ç½‘ç«™
    cn_success, cn_response_time, detected_type_cn = check_proxy_single(
        proxy, TEST_URL_CN, TIMEOUT_CN, 1, proxy_type
    )
    
    # éªŒè¯å›½é™…ç½‘ç«™  
    intl_success, intl_response_time, detected_type_intl = check_proxy_single(
        proxy, TEST_URL_INTL, TIMEOUT_INTL, 1, proxy_type
    )
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ£€æµ‹ç±»å‹ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªæ£€æµ‹ç±»å‹
    final_type = detected_type_cn if detected_type_cn != "unknown" else detected_type_intl
    if final_type == "unknown":
        final_type = proxy_type if proxy_type != "auto" else "http"
    
    return cn_success, intl_success, final_type

def check_proxies_batch(proxies, proxy_types, max_workers=MAX_WORKERS, check_type="new"):
    """
    æ‰¹é‡æ£€æŸ¥ä»£ç†IPåˆ—è¡¨ï¼ˆåŒé‡éªŒè¯ï¼‰
    """
    updated_proxies = {}
    updated_types = {}
    updated_china = {}
    updated_international = {}

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {}
        for proxy in proxies:
            # å¯¹äºå·²æœ‰ä»£ç†ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­è®°å½•çš„ç±»å‹ï¼›å¯¹äºæ–°ä»£ç†ï¼Œä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
            if check_type == "existing" and proxy in proxy_types:
                proxy_type = proxy_types[proxy]
            else:
                proxy_type = proxy_types.get(proxy, "auto")  # ä»ä¼ å…¥çš„ç±»å‹å­—å…¸è·å–
                
            future = executor.submit(check_proxy_dual, proxy, proxy_type)
            future_to_proxy[future] = proxy

        for future in concurrent.futures.as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                cn_success, intl_success, detected_type = future.result()

                # è®¡ç®—åˆ†æ•°å’Œæ›´æ–°é€»è¾‘
                current_score = proxies.get(proxy, 0)
                
                if check_type == "new":
                    # æ–°ä»£ç†ï¼šåªè¦é€šè¿‡ä»»ä¸€æµ‹è¯•å°±98åˆ†
                    if cn_success or intl_success:
                        updated_proxies[proxy] = 98
                        print(f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: {'âœ“' if cn_success else 'âœ—'} å›½é™…: {'âœ“' if intl_success else 'âœ—'}")
                    else:
                        updated_proxies[proxy] = 0
                        print(f"âŒ ä»£ç†æ— æ•ˆ: {proxy}")
                else:
                    # å·²æœ‰ä»£ç†ï¼šæ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´åˆ†æ•°
                    if cn_success and intl_success:
                        # ä¸¤æ¬¡éƒ½é€šè¿‡ï¼ŒåŠ 2åˆ†
                        updated_proxies[proxy] = min(current_score + 2, MAX_SCORE)
                        print(f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ“ å›½é™…: âœ“ | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}")
                    elif cn_success or intl_success:
                        # åªé€šè¿‡ä¸€ä¸ªï¼ŒåŠ 1åˆ†
                        updated_proxies[proxy] = min(current_score + 1, MAX_SCORE)
                        status = "å›½å†…: âœ“ å›½é™…: âœ—" if cn_success else "å›½å†…: âœ— å›½é™…: âœ“"
                        print(f"ğŸŸ¡ ä»£ç†éƒ¨åˆ†æœ‰æ•ˆ({detected_type}): {proxy} | {status} | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}")
                    else:
                        # ä¸¤ä¸ªéƒ½ä¸é€šè¿‡ï¼Œå‡1åˆ†
                        updated_proxies[proxy] = max(0, current_score - 1)
                        print(f"âŒ ä»£ç†æ— æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ— å›½é™…: âœ— | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}")
                
                # è®°å½•ç±»å‹å’Œæ”¯æŒèŒƒå›´
                updated_types[proxy] = detected_type
                updated_china[proxy] = cn_success
                updated_international[proxy] = intl_success
                        
            except Exception as e:
                print(f"âŒ é”™è¯¯ä»£ç†: {proxy} - {str(e)}")
                
                if check_type == "existing" and proxy in proxies:
                    updated_proxies[proxy] = max(0, proxies[proxy] - 1)
                else:
                    updated_proxies[proxy] = 0
                
                updated_types[proxy] = proxy_types.get(proxy, "http")
                updated_china[proxy] = False
                updated_international[proxy] = False
                    
    return updated_proxies, updated_types, updated_china, updated_international

def load_proxies_from_file(file_path):
    """ä»CSVæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨ã€ç±»å‹ã€åˆ†æ•°å’Œæ”¯æŒèŒƒå›´"""
    proxies = {}
    proxy_types = {}
    china_support = {}
    international_support = {}
    
    if not os.path.exists(file_path):
        return proxies, proxy_types, china_support, international_support

    with open(file_path, 'r', encoding="utf-8") as file:
        reader = csv.reader(file)
        for row in reader:
            if len(row) >= 5:
                # æ–°æ ¼å¼ï¼šç±»å‹,proxy:port,åˆ†æ•°,China,International
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    china = row[3].strip().lower() == 'true'
                    international = row[4].strip().lower() == 'true'
                    
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = china
                    international_support[proxy] = international
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
            elif len(row) >= 3:
                # æ—§æ ¼å¼å…¼å®¹ï¼šç±»å‹,proxy:port,åˆ†æ•°ï¼ˆé»˜è®¤ä¸æ”¯æŒä»»ä½•èŒƒå›´ï¼‰
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = False
                    international_support[proxy] = False
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
            elif len(row) >= 2:
                # æ›´æ—§æ ¼å¼å…¼å®¹ï¼šproxy:port,åˆ†æ•°ï¼ˆé»˜è®¤HTTPç±»å‹ï¼‰
                proxy = row[0].strip()
                try:
                    score = int(row[1])
                    proxies[proxy] = score
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                except:
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
    
    return proxies, proxy_types, china_support, international_support

def save_valid_proxies(proxies, proxy_types, china_support, international_support, file_path):
    """ä¿å­˜æœ‰æ•ˆä»£ç†åˆ°CSVæ–‡ä»¶ï¼ˆå¸¦ç±»å‹ã€åˆ†æ•°å’Œæ”¯æŒèŒƒå›´ï¼‰"""
    with open(file_path, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        for proxy, score in proxies.items():
            if len(proxy) > 6 and score > 0:  # åŸºæœ¬éªŒè¯
                proxy_type = proxy_types.get(proxy, "http")
                china = china_support.get(proxy, False)
                international = international_support.get(proxy, False)
                writer.writerow([proxy_type, proxy, score, china, international])

def update_proxy_scores(file_path):
    """æ›´æ–°ä»£ç†åˆ†æ•°æ–‡ä»¶ï¼Œç§»é™¤0åˆ†ä»£ç†"""
    proxies, proxy_types, china_support, international_support = load_proxies_from_file(file_path)
    valid_proxies = {k: v for k, v in proxies.items() if v > 0}
    valid_types = {k: v for k, v in proxy_types.items() if k in valid_proxies}
    valid_china = {k: v for k, v in china_support.items() if k in valid_proxies}
    valid_international = {k: v for k, v in international_support.items() if k in valid_proxies}
    save_valid_proxies(valid_proxies, valid_types, valid_china, valid_international, file_path)
    return len(proxies) - len(valid_proxies)

def filter_proxies(all_proxies):
    """ä»æ–°è·å–ä»£ç†ä¸­å»æ‰æ— æ•ˆçš„,é‡å¤çš„"""
    existing_proxies = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE,'r') as file:
            csv_reader  = csv.reader(file)
            for row in csv_reader:
                if len(row) >= 2:
                    existing_proxies.append(row[1])

    new_proxies = []
    duplicate_count = 0
    invalid_count = 0

    for proxy in all_proxies:
        try:
            if (proxy in existing_proxies) or (proxy in new_proxies):
                duplicate_count += 1
            elif (':' in proxy) and (proxy not in new_proxies):
                new_proxies.append(proxy)
            else:
                invalid_count += 1
        except:
            invalid_count += 1

    print(f'æ–°ä»£ç†:{len(new_proxies)},å·²æœ‰(é‡å¤):{duplicate_count},æ— æ•ˆ:{invalid_count}')
    return new_proxies

def validate_new_proxies(new_proxies, proxy_type="auto"):
    """éªŒè¯æ–°ä»£ç†ï¼ˆæ”¯æŒå›½å†…å¤–ï¼‰"""
    if not new_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    original_count = len(new_proxies)
    print(f"å…±åŠ è½½ {original_count} ä¸ªæ–°ä»£ç†ï¼Œä½¿ç”¨{proxy_type}ç±»å‹å¼€å§‹åŒé‡æµ‹è¯•...")
    
    new_proxies_dict = {proxy: 0 for proxy in new_proxies}
    new_types_dict = {proxy: proxy_type for proxy in new_proxies}
    
    updated_proxies, updated_types, updated_china, updated_international = check_proxies_batch(
        new_proxies_dict, new_types_dict, MAX_WORKERS, check_type="new"
    )
    
    # åˆå¹¶åˆ°ç°æœ‰ä»£ç†æ± 
    existing_proxies, existing_types, existing_china, existing_international = load_proxies_from_file(OUTPUT_FILE)
    for proxy, score in updated_proxies.items():
        if proxy not in existing_proxies or existing_proxies[proxy] < score:
            existing_proxies[proxy] = score
            existing_types[proxy] = updated_types[proxy]
            existing_china[proxy] = updated_china[proxy]
            existing_international[proxy] = updated_international[proxy]

    save_valid_proxies(existing_proxies, existing_types, existing_china, existing_international, OUTPUT_FILE)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for score in updated_proxies.values() if score == 98)
    china_only = sum(1 for proxy in updated_proxies if updated_china[proxy] and not updated_international[proxy])
    intl_only = sum(1 for proxy in updated_proxies if not updated_china[proxy] and updated_international[proxy])
    both_support = sum(1 for proxy in updated_proxies if updated_china[proxy] and updated_international[proxy])
    
    print(f"\nâœ… éªŒè¯å®Œæˆ!")
    print(f"æˆåŠŸä»£ç†: {success_count}/{original_count}")
    print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
    print(f"ä»£ç†æ± å·²æ›´æ–°è‡³: {OUTPUT_FILE}")

def validate_existing_proxies():
    """éªŒè¯å·²æœ‰ä»£ç†æ± ä¸­çš„ä»£ç†ï¼ˆæ”¯æŒå›½å†…å¤–ï¼‰"""
    print(f"å¼€å§‹éªŒè¯å·²æœ‰ä»£ç†æ± ï¼Œæ–‡ä»¶ï¼š{OUTPUT_FILE}...")
    
    # åŠ è½½ä»£ç†æ± ï¼ˆä¸åŠ è½½æ—§çš„æ”¯æŒèŒƒå›´ï¼Œä»¥æ–°éªŒè¯ç»“æœä¸ºå‡†ï¼‰
    all_proxies, proxy_types, _, _ = load_proxies_from_file(OUTPUT_FILE)
    
    if not all_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    print(f"å…±åŠ è½½ {len(all_proxies)} ä¸ªä»£ç†ï¼Œå¼€å§‹åŒé‡æµ‹è¯•...")
    
    # ä»ä»£ç†æ± ä¸­è·å–å½“å‰åˆ†æ•°å’Œç±»å‹ï¼ˆä¸è·å–æ—§çš„æ”¯æŒèŒƒå›´ï¼‰
    updated_proxies, updated_types, updated_china, updated_international = check_proxies_batch(
        all_proxies, proxy_types, MAX_WORKERS, "existing"
    )
    
    # æ›´æ–°æ‰€æœ‰ä»£ç†åˆ†æ•°å’Œæ”¯æŒèŒƒå›´
    for proxy, score in updated_proxies.items():
        all_proxies[proxy] = score
        proxy_types[proxy] = updated_types[proxy]
    
    # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
    save_valid_proxies(all_proxies, proxy_types, updated_china, updated_international, OUTPUT_FILE)
    
    # æ¸…ç†0åˆ†ä»£ç†
    removed_count = update_proxy_scores(OUTPUT_FILE)
    
    # æœ€ç»ˆç»Ÿè®¡
    final_proxies, _, final_china, final_international = load_proxies_from_file(OUTPUT_FILE)
    final_count = len(final_proxies)
    
    china_only = sum(1 for proxy in final_proxies if final_china[proxy] and not final_international[proxy])
    intl_only = sum(1 for proxy in final_proxies if not final_china[proxy] and final_international[proxy])
    both_support = sum(1 for proxy in final_proxies if final_china[proxy] and final_international[proxy])

    print(f"\néªŒè¯å®Œæˆ! å‰©ä½™æœ‰æ•ˆä»£ç†: {final_count}/{len(all_proxies)}")
    print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
    print(f"å·²ç§»é™¤ {removed_count} ä¸ªæ— æ•ˆä»£ç†")

def main(scraper_choice):
    all_proxies = []  # å­˜å‚¨æ‰€æœ‰çˆ¬å–çš„ä»£ç†
    by_type = ''  # é€šè¿‡æŒ‡å®šç±»å‹éªŒè¯,é»˜è®¤ä¸ºå¦

    if scraper_choice == "1":
        try:
            by_type = 'http'   # é»˜è®¤ç”¨http
            count = 1000

            print(f"å¼€å§‹çˆ¬å– {count} ä¸ªä»£ç†...")
            try:
                # é€‚åˆç”¨åç¨‹
                import aiohttp
                import asyncio
                
                async def fetch_proxy(session, url, semaphore):
                    async with semaphore:
                        try:
                            async with session.get(url) as response:
                                if response.status == 200:
                                    proxy = await response.text()
                                    print(proxy)
                                    return proxy.strip()
                        except:
                            return None
                
                async def fetch_proxies_main():
                    semaphore = asyncio.Semaphore(20)   # æœ€å¤§å¹¶å‘
                    timeout = aiohttp.ClientTimeout(total=50)   # è¶…æ—¶(ç»™æœåŠ¡å™¨è¶³å¤Ÿå“åº”æ—¶é—´)
                    
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        tasks = []
                        for _ in range(count):
                            url = 'https://proxypool.scrape.center/random'
                            task = fetch_proxy(session, url, semaphore)
                            tasks.append(task)
                        
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        return [r for r in results if r and isinstance(r, str) and ':' in r]
                
                proxies = asyncio.run(fetch_proxies_main())
                if proxies:
                    all_proxies.extend(proxies)
                    
            except ImportError:
                print("aiohttp æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥è¯·æ±‚...")
                # åŒæ­¥å¤‡é€‰æ–¹æ¡ˆ
                for _ in range(count):
                    try:
                        proxy = requests.get('https://proxypool.scrape.center/random', timeout=30).text.strip()
                        if proxy and ':' in proxy:
                            all_proxies.append(proxy)
                    except:
                        continue

            print(f"çˆ¬å–å®Œæˆï¼")
        except ValueError:
            print("é”™è¯¯")
            return None, None
    
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt
    elif scraper_choice == '2':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '3':
        by_type = 'socks5'   # é»˜è®¤ç”¨socks5
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '4':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks4.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt
    elif scraper_choice == '5':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '6':
        by_type = 'http'   # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '7':
        by_type = 'socks4'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '8':
        by_type = 'socks5'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = re.sub(r':\D.*?\n','\n',response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt
      
    elif scraper_choice == '9':
        by_type = 'socks5'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url,headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    return filter_proxies(all_proxies) , by_type

if __name__ == '__main__':
    # ä¸»æµç¨‹ï¼šçˆ¬å– -> éªŒè¯æ–°ä»£ç† -> éªŒè¯å·²æœ‰ä»£ç†
    print("=== å¼€å§‹ä»£ç†çˆ¬å–å’ŒéªŒè¯æµç¨‹ ===")
    
    for i in range(1,9+1):
        # 1. çˆ¬å–æ–°ä»£ç†
        new_proxies,by_type = main(scraper_choice=str(i))

        # 2. éªŒè¯æ–°ä»£ç†
        if new_proxies:
            if by_type:
                validate_new_proxies(new_proxies,by_type)
            else:
                validate_new_proxies(new_proxies, "auto")

    # 3. å…¨çˆ¬å®ŒåéªŒè¯å·²æœ‰ä»£ç†
    validate_existing_proxies()
    
    print("=== æµç¨‹å®Œæˆ ===")
