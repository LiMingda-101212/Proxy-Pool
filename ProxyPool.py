import re
import requests
import concurrent.futures
import time
import os
import csv

# ============é…ç½®åŒº
OUTPUT_FILE = "proxies.csv"  # è¾“å‡ºæœ‰æ•ˆä»£ç†æ–‡ä»¶
TEST_URL_CN = "http://www.baidu.com"  # å›½å†…æµ‹è¯•URL
TEST_URL_INTL = "http://www.google.com"  # å›½é™…æµ‹è¯•URL
TRANSPARENT_CHECK_URL = "http://httpbin.org/ip"  # é€æ˜ä»£ç†æ£€æµ‹URL
TIMEOUT_CN = 6  # å›½å†…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)
TIMEOUT_INTL = 10  # å›½é™…æµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)
TRANSPARENT_TIMEOUT = 8  # é€æ˜ä»£ç†æ£€æµ‹è¶…æ—¶æ—¶é—´(ç§’)
MAX_WORKERS = 50  # æœ€å¤§å¹¶å‘æ•°
MAX_SCORE = 100  # æœ€å¤§ç§¯åˆ†


class ProxyScraper:
    """é€šç”¨ä»£ç†çˆ¬å–å™¨ - é€‚åˆä¸€äº›éœ€è§£æçš„ç½‘ç«™"""

    def __init__(self, url: str, regex_pattern: str, capture_groups: list):
        self.url = url
        self.headers = {
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0'
        }
        self.encoding = "utf-8"
        self.regex_pattern = regex_pattern
        self.capture_groups = capture_groups

    def scrape_proxies(self):
        extracted_data = []
        try:
            response = requests.get(url=self.url, headers=self.headers, timeout=TIMEOUT_CN)
            if response.status_code == 200:
                response.encoding = self.encoding
                regex = re.compile(self.regex_pattern, re.S)
                matches = regex.finditer(response.text)
                for match in matches:
                    for group_name in self.capture_groups:
                        extracted_data.append(f"{match.group(group_name)}")
                proxy_list = [f"{extracted_data[i].strip()}:{extracted_data[i + 1].strip()}"
                              for i in range(0, len(extracted_data), 2)]
                response.close()
                return proxy_list
            else:
                print(f"çˆ¬å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥ï¼Œé”™è¯¯: {str(e)}")
            return []


def get_own_ip():
    """è·å–è‡ªå·±çš„å…¬ç½‘IPåœ°å€"""
    try:
        response = requests.get(TRANSPARENT_CHECK_URL, timeout=TRANSPARENT_TIMEOUT)
        if response.status_code == 200:
            return response.json()['origin']
    except Exception as e:
        print(f"è·å–æœ¬æœºIPå¤±è´¥: {str(e)}")
    return None


def check_transparent_proxy(proxy, proxy_type="http", own_ip=None):
    """
    æ£€æµ‹ä»£ç†æ˜¯å¦ä¸ºé€æ˜ä»£ç†

    :param proxy: ä»£ç†åœ°å€
    :param proxy_type: ä»£ç†ç±»å‹
    :param own_ip: è‡ªå·±çš„å…¬ç½‘IPï¼ˆå¯é€‰ï¼‰
    :return: (æ˜¯å¦ä¸ºé€æ˜ä»£ç†, æ£€æµ‹åˆ°çš„IP)
    """
    if own_ip is None:
        own_ip = get_own_ip()
        if own_ip is None:
            return False, "unknown"  # æ— æ³•è·å–æœ¬æœºIPï¼Œè·³è¿‡é€æ˜ä»£ç†æ£€æµ‹

    try:
        # è®¾ç½®ä»£ç†
        if proxy_type == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif proxy_type == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif proxy_type == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }

        # ä½¿ç”¨ä»£ç†è®¿é—®æ£€æµ‹ç½‘ç«™
        response = requests.get(
            TRANSPARENT_CHECK_URL,
            proxies=proxies_config,
            timeout=TRANSPARENT_TIMEOUT
        )

        if response.status_code == 200:
            proxy_ip_data = response.json()
            proxy_ip = proxy_ip_data['origin']

            # åˆ¤æ–­æ˜¯å¦ä¸ºé€æ˜ä»£ç†ï¼šå¦‚æœè¿”å›çš„IPåŒ…å«çœŸå®IPï¼Œåˆ™ä¸ºé€æ˜ä»£ç†
            is_transparent = own_ip in proxy_ip

            return is_transparent, proxy_ip
        else:
            return False, "unknown"

    except Exception as e:
        return False, "unknown"


def check_proxy_single(proxy, test_url, timeout=TIMEOUT_CN,
                       retries=1, proxy_type="auto"):
    """
    æ£€æŸ¥å•ä¸ªä»£ç†IPå¯¹å•ä¸ªURLçš„å¯ç”¨æ€§
    """
    if proxy_type == "auto":
        protocols_to_try = ["http", "socks5", "socks4"]
    else:
        protocols_to_try = [proxy_type]

    detected_type = proxy_type if proxy_type != "auto" else "unknown"

    for current_protocol in protocols_to_try:
        if current_protocol == "http":
            proxies_config = {
                "http": f"http://{proxy}",
                "https": f"http://{proxy}"
            }
        elif current_protocol == "socks4":
            proxies_config = {
                "http": f"socks4://{proxy}",
                "https": f"socks4://{proxy}"
            }
        elif current_protocol == "socks5":
            proxies_config = {
                "http": f"socks5://{proxy}",
                "https": f"socks5://{proxy}"
            }
        else:
            continue

        for attempt in range(retries):
            try:
                start_time = time.time()
                response = requests.get(
                    test_url,
                    proxies=proxies_config,
                    timeout=timeout,
                    allow_redirects=False
                )
                end_time = time.time()
                response_time = end_time - start_time

                if response_time > timeout:
                    break

                if response.status_code == 200:
                    detected_type = current_protocol
                    return True, response_time, detected_type

            except Exception:
                if attempt < retries - 1:
                    time.sleep(0.5)
                    continue
                break

    if proxy_type != "auto":
        detected_type = proxy_type

    return False, None, detected_type


def check_proxy_dual(proxy, proxy_type="auto"):
    """
    åŒé‡éªŒè¯ä»£ç†ï¼šåŒæ—¶éªŒè¯ç™¾åº¦(å›½å†…)å’ŒGoogle(å›½é™…)ï¼Œå¹¶è¿›è¡Œé€æ˜ä»£ç†æ£€æµ‹

    :return: (æ˜¯å¦é€šè¿‡å›½å†…, æ˜¯å¦é€šè¿‡å›½é™…, æœ€ç»ˆæ£€æµ‹ç±»å‹, æ˜¯å¦ä¸ºé€æ˜ä»£ç†, æ£€æµ‹åˆ°çš„IP)
    """
    # éªŒè¯å›½å†…ç½‘ç«™
    cn_success, cn_response_time, detected_type_cn = check_proxy_single(
        proxy, TEST_URL_CN, TIMEOUT_CN, 1, proxy_type
    )

    # éªŒè¯å›½é™…ç½‘ç«™
    intl_success, intl_response_time, detected_type_intl = check_proxy_single(
        proxy, TEST_URL_INTL, TIMEOUT_INTL, 1, proxy_type
    )

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„æ£€æµ‹ç±»å‹ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªæ£€æµ‹ç±»å‹
    final_type = detected_type_cn if detected_type_cn != "unknown" else detected_type_intl
    if final_type == "unknown":
        final_type = proxy_type if proxy_type != "auto" else "http"

    # é€æ˜ä»£ç†æ£€æµ‹ï¼ˆåªåœ¨ä»£ç†æœ‰æ•ˆæ—¶è¿›è¡Œï¼‰
    is_transparent = False
    detected_ip = "unknown"

    if cn_success or intl_success:
        is_transparent, detected_ip = check_transparent_proxy(proxy, final_type)

    return cn_success, intl_success, final_type, is_transparent, detected_ip


def check_proxies_batch(proxies, proxy_types, max_workers=MAX_WORKERS, check_type="new"):
    """
    æ‰¹é‡æ£€æŸ¥ä»£ç†IPåˆ—è¡¨ï¼ˆåŒé‡éªŒè¯ + é€æ˜ä»£ç†æ£€æµ‹ï¼‰
    """
    updated_proxies = {}
    updated_types = {}
    updated_china = {}
    updated_international = {}
    updated_transparent = {}
    updated_detected_ips = {}

    # é¢„å…ˆè·å–æœ¬æœºIPç”¨äºé€æ˜ä»£ç†æ£€æµ‹
    own_ip = get_own_ip()
    if own_ip is None:
        print("âš ï¸  æ— æ³•è·å–æœ¬æœºIPï¼Œè·³è¿‡é€æ˜ä»£ç†æ£€æµ‹")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {}
        for proxy in proxies:
            # å¯¹äºå·²æœ‰ä»£ç†ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­è®°å½•çš„ç±»å‹ï¼›å¯¹äºæ–°ä»£ç†ï¼Œå…ˆçœ‹æœ‰æ²¡æœ‰æŒ‡å®š,å¦åˆ™ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
            if check_type == "existing" and proxy in proxy_types:
                proxy_type = proxy_types[proxy]
            else:
                proxy_type = proxy_types.get(proxy, "auto")  # ä»ä¼ å…¥çš„ç±»å‹å­—å…¸è·å–

            future = executor.submit(check_proxy_dual, proxy, proxy_type)
            future_to_proxy[future] = proxy

        for future in concurrent.futures.as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                cn_success, intl_success, detected_type, is_transparent, detected_ip = future.result()

                # è®¡ç®—åˆ†æ•°å’Œæ›´æ–°é€»è¾‘
                current_score = proxies.get(proxy, 0)

                if check_type == "new":
                    # æ–°ä»£ç†ï¼šåªè¦é€šè¿‡ä»»ä¸€æµ‹è¯•å°±98åˆ†
                    if cn_success or intl_success:
                        updated_proxies[proxy] = 98
                        # é€æ˜ä»£ç†è­¦å‘Š
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(
                            f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: {'âœ“' if cn_success else 'âœ—'} å›½é™…: {'âœ“' if intl_success else 'âœ—'}{transparent_warning}")
                    else:
                        updated_proxies[proxy] = 0
                        print(f"âŒ ä»£ç†æ— æ•ˆ: {proxy}")
                else:
                    # å·²æœ‰ä»£ç†ï¼šæ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´åˆ†æ•°
                    if cn_success and intl_success:
                        # ä¸¤æ¬¡éƒ½é€šè¿‡ï¼ŒåŠ 2åˆ†
                        updated_proxies[proxy] = min(current_score + 2, MAX_SCORE)
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(
                            f"âœ… ä»£ç†æœ‰æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ“ å›½é™…: âœ“ | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}{transparent_warning}")
                    elif cn_success or intl_success:
                        # åªé€šè¿‡ä¸€ä¸ªï¼ŒåŠ 1åˆ†
                        updated_proxies[proxy] = min(current_score + 1, MAX_SCORE)
                        status = "å›½å†…: âœ“ å›½é™…: âœ—" if cn_success else "å›½å†…: âœ— å›½é™…: âœ“"
                        transparent_warning = " âš ï¸ é€æ˜ä»£ç†" if is_transparent else ""
                        print(
                            f"ğŸŸ¡ ä»£ç†éƒ¨åˆ†æœ‰æ•ˆ({detected_type}): {proxy} | {status} | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}{transparent_warning}")
                    else:
                        # ä¸¤ä¸ªéƒ½ä¸é€šè¿‡ï¼Œå‡1åˆ†
                        updated_proxies[proxy] = max(0, current_score - 1)
                        print(
                            f"âŒ ä»£ç†æ— æ•ˆ({detected_type}): {proxy} | å›½å†…: âœ— å›½é™…: âœ— | åˆ†æ•°: {current_score} -> {updated_proxies[proxy]}")

                # è®°å½•ç±»å‹å’Œæ”¯æŒèŒƒå›´
                updated_types[proxy] = detected_type
                updated_china[proxy] = cn_success
                updated_international[proxy] = intl_success
                updated_transparent[proxy] = is_transparent
                updated_detected_ips[proxy] = detected_ip

            except Exception as e:
                print(f"âŒ é”™è¯¯ä»£ç†: {proxy} - {str(e)}")

                if check_type == "existing" and proxy in proxies:
                    updated_proxies[proxy] = max(0, proxies[proxy] - 1)
                else:
                    updated_proxies[proxy] = 0

                updated_types[proxy] = proxy_types.get(proxy, "http")
                updated_china[proxy] = False
                updated_international[proxy] = False
                updated_transparent[proxy] = False
                updated_detected_ips[proxy] = "unknown"

    return updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips


def load_proxies_from_file(file_path):
    """ä»CSVæ–‡ä»¶åŠ è½½ä»£ç†åˆ—è¡¨ã€ç±»å‹ã€åˆ†æ•°ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯
    :param file_path: csvä»£ç†æ± æ–‡ä»¶
    Returns:
        proxies: ä»£ç†å’Œåˆ†æ•° -> {'180.167.238.98:7302': 100, '123.128.12.93:9050': 81}

        proxy_types: ä»£ç†å’Œç±»å‹ -> {'180.167.238.98:7302': 'http', '123.128.12.93:9050': 'http'}

        china_support: ä»£ç†å’Œæ˜¯å¦æ”¯æŒä¸­å›½ -> {'180.167.238.98:7302': False, '123.128.12.93:9050': False}

        international_support: ä»£ç†å’Œæ˜¯å¦æ”¯æŒå›½é™… -> {'180.167.238.98:7302': False, '123.128.12.93:9050': False}

        transparent_proxies: ä»£ç†å’Œæ˜¯å¦ä¸ºé€æ˜ä»£ç† -> {'180.167.238.98:7302': False, '123.128.12.93:9050': False}

        detected_ips: ä»£ç†å’Œæ£€æµ‹åˆ°çš„ip -> {'164.163.42.46:10000': 'unknown', '176.100.216.164:8282': 'unknown'}

        browser_valid_status: ä»£ç†å’Œæµè§ˆå™¨æ˜¯å¦å¯ç”¨

        browser_check_dates: ä»£ç†å’Œæµè§ˆå™¨æœ€åæ£€æŸ¥æ—¥æœŸ
    """
    proxies = {}
    proxy_types = {}
    china_support = {}
    international_support = {}
    transparent_proxies = {}
    detected_ips = {}
    browser_valid_status = {}
    browser_check_dates = {}

    if not os.path.exists(file_path):
        return proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips, browser_valid_status, browser_check_dates

    with open(file_path, 'r', encoding="utf-8") as file:
        reader = csv.reader(file)
        for row in reader:
            if len(row) >= 9:
                # æ ¼å¼: ç±»å‹,proxy:port,åˆ†æ•°,China,International,Transparent,DetectedIP,Browser_Valid,Browser_Date
                proxy_type = row[0].strip().lower()
                proxy = row[1].strip()
                try:
                    score = int(row[2])
                    china = row[3].strip().lower() == 'true'
                    international = row[4].strip().lower() == 'true'
                    transparent = row[5].strip().lower() == 'true'
                    detected_ip = row[6].strip() if len(row) > 6 else "unknown"
                    browser_valid = row[7].strip().lower() if len(row) > 7 else "unknown"
                    browser_date = row[8].strip() if len(row) > 8 else "unknown"

                    proxies[proxy] = score
                    proxy_types[proxy] = proxy_type
                    china_support[proxy] = china
                    international_support[proxy] = international
                    transparent_proxies[proxy] = transparent
                    detected_ips[proxy] = detected_ip
                    browser_valid_status[proxy] = browser_valid
                    browser_check_dates[proxy] = browser_date

                except Exception as e:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    proxies[proxy] = 70
                    proxy_types[proxy] = "http"
                    china_support[proxy] = False
                    international_support[proxy] = False
                    transparent_proxies[proxy] = False
                    detected_ips[proxy] = "unknown"
                    browser_valid_status[proxy] = "unknown"
                    browser_check_dates[proxy] = "unknown"

    return proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips, browser_valid_status, browser_check_dates

def save_valid_proxies(proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips, browser_valid_status, browser_check_dates, file_path):
    """ä¿å­˜æœ‰æ•ˆä»£ç†åˆ°CSVæ–‡ä»¶ï¼ˆå¸¦ç±»å‹ã€åˆ†æ•°ã€æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼‰"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)

    with open(file_path, 'w', encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        for proxy, score in proxies.items():
            if len(proxy) > 7 and score > 0:  # åŸºæœ¬éªŒè¯
                proxy_type = proxy_types.get(proxy, "http")
                china = china_support.get(proxy, False)
                international = international_support.get(proxy, False)
                transparent = transparent_proxies.get(proxy, False)
                detected_ip = detected_ips.get(proxy, "unknown")
                browser_valid_value = browser_valid_status.get(proxy, "unknown")
                browser_check_date = browser_check_dates.get(proxy, "unknown")
                writer.writerow([proxy_type, proxy, score, china, international, transparent, detected_ip,browser_valid_value,browser_check_date])


def update_proxy_scores(file_path):
    """æ›´æ–°ä»£ç†åˆ†æ•°æ–‡ä»¶ï¼Œç§»é™¤0åˆ†ä»£ç†"""
    proxies, proxy_types, china_support, international_support, transparent_proxies, detected_ips, browser_valid_status, browser_check_dates = load_proxies_from_file(file_path)
    valid_proxies = {k: v for k, v in proxies.items() if v > 0}
    valid_types = {k: v for k, v in proxy_types.items() if k in valid_proxies}
    valid_china = {k: v for k, v in china_support.items() if k in valid_proxies}
    valid_international = {k: v for k, v in international_support.items() if k in valid_proxies}
    valid_transparent = {k: v for k, v in transparent_proxies.items() if k in valid_proxies}
    valid_detected_ips = {k: v for k, v in detected_ips.items() if k in valid_proxies}
    valid_browser_status = {k: v for k, v in browser_valid_status.items() if k in valid_proxies}
    valid_browser_check_dates = {k: v for k, v in browser_check_dates.items() if k in valid_proxies}

    save_valid_proxies(valid_proxies, valid_types, valid_china, valid_international, valid_transparent, valid_detected_ips, valid_browser_status, valid_browser_check_dates, file_path)
    return len(proxies) - len(valid_proxies)


def filter_proxies(all_proxies):
    """ä»æ–°è·å–ä»£ç†ä¸­å»æ‰æ— æ•ˆçš„,é‡å¤çš„"""
    existing_proxies = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r') as file:
            csv_reader = csv.reader(file)
            for row in csv_reader:
                if len(row) >= 2:
                    existing_proxies.append(row[1])

    new_proxies = []
    duplicate_count = 0
    invalid_count = 0

    for proxy in all_proxies:
        try:
            if (proxy in existing_proxies) or (proxy in new_proxies):
                duplicate_count += 1
            elif (':' in proxy) and (proxy not in new_proxies):
                new_proxies.append(proxy)
            else:
                invalid_count += 1
        except:
            invalid_count += 1

    print(f'æ–°ä»£ç†:{len(new_proxies)},å·²æœ‰(é‡å¤):{duplicate_count},æ— æ•ˆ:{invalid_count}')
    return new_proxies


def validate_new_proxies(new_proxies, proxy_type="auto"):
    """éªŒè¯æ–°ä»£ç†ï¼ˆæ”¯æŒå›½å†…å¤–å’Œé€æ˜ä»£ç†æ£€æµ‹ï¼‰"""
    if not new_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    original_count = len(new_proxies)
    print(f"å…±åŠ è½½ {original_count} ä¸ªæ–°ä»£ç†ï¼Œä½¿ç”¨{proxy_type}ç±»å‹å¼€å§‹å›½å†…å¤–åŒé‡æµ‹è¯•...")
    print("å¯ç”¨é€æ˜ä»£ç†æ£€æµ‹")

    new_proxies_dict = {proxy: 0 for proxy in new_proxies}
    new_types_dict = {proxy: proxy_type for proxy in new_proxies}

    updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips = check_proxies_batch(
        new_proxies_dict, new_types_dict, MAX_WORKERS, check_type="new"
    )

    # åˆå¹¶åˆ°ç°æœ‰ä»£ç†æ± 
    existing_proxies, existing_types, existing_china, existing_international, existing_transparent, existing_detected_ips, browser_valid_status, browser_check_dates = load_proxies_from_file(
        OUTPUT_FILE)

    for proxy, score in updated_proxies.items():
        if proxy not in existing_proxies or existing_proxies[proxy] < score:
            existing_proxies[proxy] = score
            existing_types[proxy] = updated_types[proxy]
            existing_china[proxy] = updated_china[proxy]
            existing_international[proxy] = updated_international[proxy]
            existing_transparent[proxy] = updated_transparent[proxy]
            existing_detected_ips[proxy] = updated_detected_ips[proxy]
        # æµè§ˆå™¨éªŒè¯éƒ¨åˆ†ä¸å˜,ç›´æ¥ä¼ å…¥
    save_valid_proxies(existing_proxies, existing_types, existing_china, existing_international, existing_transparent,
                       existing_detected_ips, browser_valid_status, browser_check_dates, OUTPUT_FILE)
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for score in updated_proxies.values() if score == 98)
    china_only = sum(1 for proxy in updated_proxies if updated_china[proxy] and not updated_international[proxy])
    intl_only = sum(1 for proxy in updated_proxies if not updated_china[proxy] and updated_international[proxy])
    both_support = sum(1 for proxy in updated_proxies if updated_china[proxy] and updated_international[proxy])
    transparent_count = sum(1 for proxy in updated_proxies if updated_transparent[proxy])

    print(f"\nâœ… éªŒè¯å®Œæˆ!")
    print(f"æˆåŠŸä»£ç†: {success_count}/{original_count}")
    print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
    print(f"âš ï¸  é€æ˜ä»£ç†: {transparent_count} ä¸ª")
    print(f"ä»£ç†æ± å·²æ›´æ–°è‡³: {OUTPUT_FILE}")


def validate_existing_proxies():
    """éªŒè¯å·²æœ‰ä»£ç†æ± ä¸­çš„ä»£ç†ï¼ˆæ”¯æŒå›½å†…å¤–å’Œé€æ˜ä»£ç†æ£€æµ‹ï¼‰"""
    print(f"å¼€å§‹éªŒè¯å·²æœ‰ä»£ç†æ± ï¼Œæ–‡ä»¶ï¼š{OUTPUT_FILE}...")
    print("ğŸ” å¯ç”¨é€æ˜ä»£ç†æ£€æµ‹")

    # åŠ è½½ä»£ç†æ± ï¼ˆä¸åŠ è½½æ—§çš„æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼Œä»¥æ–°éªŒè¯ç»“æœä¸ºå‡†ï¼‰
    all_proxies, proxy_types, _, _, _, _,browser_valid_status, browser_check_dates = load_proxies_from_file(OUTPUT_FILE)

    if not all_proxies:
        print("æ²¡æœ‰ä»£ç†éœ€è¦éªŒè¯")
        return

    print(f"å…±åŠ è½½ {len(all_proxies)} ä¸ªä»£ç†ï¼Œå¼€å§‹åŒé‡æµ‹è¯•...")

    # ä»ä»£ç†æ± ä¸­è·å–å½“å‰åˆ†æ•°å’Œç±»å‹ï¼ˆä¸è·å–æ—§çš„æ”¯æŒèŒƒå›´å’Œé€æ˜ä»£ç†ä¿¡æ¯ï¼‰
    updated_proxies, updated_types, updated_china, updated_international, updated_transparent, updated_detected_ips = check_proxies_batch(
        all_proxies, proxy_types, MAX_WORKERS, "existing"
    )

    # æ›´æ–°æ‰€æœ‰ä»£ç†åˆ†æ•°å’Œæ”¯æŒèŒƒå›´
    for proxy, score in updated_proxies.items():
        all_proxies[proxy] = score
        proxy_types[proxy] = updated_types[proxy]

    # ä¿å­˜æ›´æ–°åçš„ä»£ç†æ± 
    save_valid_proxies(all_proxies, proxy_types, updated_china, updated_international, updated_transparent, updated_detected_ips, browser_valid_status, browser_check_dates, OUTPUT_FILE)

    # æ¸…ç†0åˆ†ä»£ç†
    removed_count = update_proxy_scores(OUTPUT_FILE)

    # æœ€ç»ˆç»Ÿè®¡
    final_proxies, _, final_china, final_international, final_transparent, _,_,_ = load_proxies_from_file(OUTPUT_FILE)
    final_count = len(final_proxies)

    china_only = sum(1 for proxy in final_proxies if final_china[proxy] and not final_international[proxy])
    intl_only = sum(1 for proxy in final_proxies if not final_china[proxy] and final_international[proxy])
    both_support = sum(1 for proxy in final_proxies if final_china[proxy] and final_international[proxy])
    transparent_count = sum(1 for proxy in final_proxies if final_transparent[proxy])

    print(f"\néªŒè¯å®Œæˆ! å‰©ä½™æœ‰æ•ˆä»£ç†: {final_count}/{len(all_proxies)}")
    print(f"ä»…æ”¯æŒå›½å†…: {china_only} | ä»…æ”¯æŒå›½é™…: {intl_only} | åŒæ”¯æŒ: {both_support}")
    print(f"âš ï¸  é€æ˜ä»£ç†: {transparent_count} ä¸ª")
    print(f"å·²ç§»é™¤ {removed_count} ä¸ªæ— æ•ˆä»£ç†")


def main(scraper_choice):
    all_proxies = []  # å­˜å‚¨æ‰€æœ‰çˆ¬å–çš„ä»£ç†
    by_type = ''  # é€šè¿‡æŒ‡å®šç±»å‹éªŒè¯,é»˜è®¤ä¸ºå¦

    if scraper_choice == "1":
        try:
            by_type = 'http'  # é»˜è®¤ç”¨http
            count = 1000

            print(f"å¼€å§‹çˆ¬å– {count} ä¸ªä»£ç†...")
            try:
                # é€‚åˆç”¨åç¨‹
                import aiohttp
                import asyncio

                async def fetch_proxy(session, url, semaphore):
                    async with semaphore:
                        try:
                            async with session.get(url) as response:
                                if response.status == 200:
                                    proxy = await response.text()
                                    print(proxy)
                                    return proxy.strip()
                        except:
                            return None

                async def fetch_proxies_main():
                    semaphore = asyncio.Semaphore(20)  # æœ€å¤§å¹¶å‘
                    timeout = aiohttp.ClientTimeout(total=50)  # è¶…æ—¶(ç»™æœåŠ¡å™¨è¶³å¤Ÿå“åº”æ—¶é—´)

                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        tasks = []
                        for _ in range(count):
                            url = 'https://proxypool.scrape.center/random'
                            task = fetch_proxy(session, url, semaphore)
                            tasks.append(task)

                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        return [r for r in results if r and isinstance(r, str) and ':' in r]

                proxies = asyncio.run(fetch_proxies_main())
                if proxies:
                    all_proxies.extend(proxies)

            except ImportError:
                print("aiohttp æœªå®‰è£…ï¼Œä½¿ç”¨åŒæ­¥è¯·æ±‚...")
                # åŒæ­¥å¤‡é€‰æ–¹æ¡ˆ
                for _ in range(count):
                    try:
                        proxy = requests.get('https://proxypool.scrape.center/random', timeout=30).text.strip()
                        if proxy and ':' in proxy:
                            all_proxies.append(proxy)
                    except:
                        continue

            print(f"çˆ¬å–å®Œæˆï¼")
        except ValueError:
            print("é”™è¯¯")
            return None, None

    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt
    # https://github.com/databay-labs/free-proxy-list/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt
    elif scraper_choice == '2':
        by_type = 'http'  # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/http.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '3':
        by_type = 'socks5'  # é»˜è®¤ç”¨socks5
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '4':
        by_type = 'http'  # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/databay-labs/free-proxy-list/refs/heads/master/https.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/http.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/https.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks4.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt
    # https://github.com/zloi-user/hideip.me/raw/refs/heads/master/socks5.txt -> https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt
    elif scraper_choice == '5':
        by_type = 'http'  # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/http.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = re.sub(r':\D.*?\n', '\n', response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '6':
        by_type = 'http'  # é»˜è®¤ç”¨http
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/https.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = re.sub(r':\D.*?\n', '\n', response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '7':
        by_type = 'socks4'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks4.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = re.sub(r':\D.*?\n', '\n', response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    elif scraper_choice == '8':
        by_type = 'socks5'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/zloi-user/hideip.me/refs/heads/master/socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = re.sub(r':\D.*?\n', '\n', response.text).split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    # https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt

    elif scraper_choice == '9':
        by_type = 'socks5'
        print('å¼€å§‹çˆ¬å–:https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt')
        error_count = 0
        url = 'https://raw.githubusercontent.com/r00tee/Proxy-List/main/Socks5.txt'
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        try:
            response = requests.get(url, headers=headers)
            result = response.text.split("\n")
            proxy_list = []
            for proxy in result:
                if len(result) == 0:
                    print('æ²¡æœ‰ä»£ç†å¯ä»¥çˆ¬å–')
                else:
                    proxy_list.append(proxy.strip())
            if isinstance(proxy_list, list):
                all_proxies.extend(proxy_list)
            else:
                error_count += 1
            print(f'1/1  é”™è¯¯æ•°:{error_count}')
        except Exception as e:
            print(f'çˆ¬å–å¤±è´¥: {str(e)}')

    return filter_proxies(all_proxies), by_type


if __name__ == '__main__':
    # ä¸»æµç¨‹ï¼šçˆ¬å– -> éªŒè¯æ–°ä»£ç† -> éªŒè¯å·²æœ‰ä»£ç†
    print("=== å¼€å§‹ä»£ç†çˆ¬å–å’ŒéªŒè¯æµç¨‹ ===")

    for i in range(1, 9 + 1):
        # 1. çˆ¬å–æ–°ä»£ç†
        new_proxies, by_type = main(scraper_choice=str(i))

        # 2. éªŒè¯æ–°ä»£ç†
        if new_proxies:
            if by_type:
                validate_new_proxies(new_proxies, by_type)
            else:
                validate_new_proxies(new_proxies, "auto")

    # 3. å…¨çˆ¬å®ŒåéªŒè¯å·²æœ‰ä»£ç†
    validate_existing_proxies()

    print("=== æµç¨‹å®Œæˆ ===")
